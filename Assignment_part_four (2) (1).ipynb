{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03mjfOsnIH1t"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Assignements/Part%204/Assignment_part_four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZu-7QbP9muh"
      },
      "source": [
        "DSML investigation:\n",
        "\n",
        "You are part of the Suisse Impossible Mission Force, or SIMF for short. You need to uncover a rogue agent that is trying to steal sensitive information.\n",
        "\n",
        "Your mission, should you choose to accept it, is to find that agent before stealing any classified information. Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyL7WNdV9sWV"
      },
      "source": [
        "# Assignement part four\n",
        "### Due 22.10\n",
        "#### Identifying the suspects credit score\n",
        "We received informations that the rogue agent has a good credit score.\n",
        "\n",
        "Our spies at SIMF have managed to collect financial information relating to our suspects as well as a training dataset.\n",
        "\n",
        "Create a Neural Network over the training dataset `df` to identify which of the suspects have a good Credit_Mix\n",
        "\n",
        "\n",
        "## Getting to know our data\n",
        "\n",
        "* Age: a users age\n",
        "* Occupation: a users employment field\n",
        "* Annual_Income: a users annual income\n",
        "* Monthly_Inh_Salary: the calculated salary received by a given user on a monthly basis\n",
        "* Num_Bank_Accounts: the number of bank accounts possessed by a given user\n",
        "* Num_Credit_Cards: the number of credit card given user possesses\n",
        "* Interest_Rate: The interest rate on those cards (if multiple then its the average)\n",
        "* Num_of_Loans: The number of loans of each user\n",
        "* Delay_from_due_date: payment tardiness of user\n",
        "* Num_of_Delayed_Payment: the count of delayed payments\n",
        "* Changed_Credit_Limit: NaN\n",
        "* Num_Credit_Inquiries: NaN\n",
        "* Credit_Mix: The users credit score\n",
        "* Outsting_Debt: Outstanding debt\n",
        "* Credit_Utilization_Ratio: the percentage of borrowed money over borrowing allowance\n",
        "* Payment_of_Min_Amount: does the user usually pay the minimal amount (categorical)\n",
        "* Total_EMI_per_month: Monthly repayments to be made\n",
        "* Amount_invested_monthly: The amout put in an investment fun by the user on a monthly basis\n",
        "* Payment_Behaviour: the users payment behavior (categorical)\n",
        "* Monthly_Balance: The users end of the month balance\n",
        "* AutoLoan: If the user has an active loan for their vehicule\n",
        "* Credit-BuilderLoan: If the user has a loan to increase their credit score\n",
        "* DebtConsolidationLoan, HomeEquityLoan, MortgageLoan, NotSpecified, PaydayLoan, PersonalLoan, StudentLoan: different types of loans(categorical features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "XHhI95r5-tyD"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2qxwPjzsIH11"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/train_classification.csv\", index_col='Unnamed: 0').dropna()\n",
        "suspects = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/suspects.csv\", index_col='Unnamed: 0').dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "D-Gp1WqkIH11",
        "outputId": "b21302cf-02ce-42ef-9c66-7f8df7750e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Age Occupation  Annual_Income  Monthly_Inh_Salary  Num_Bank_Accounts  \\\n",
              "0   23  Scientist       19114.12         1824.843333                  3   \n",
              "1   24  Scientist       19114.12         1824.843333                  3   \n",
              "3   24  Scientist       19114.12         4182.004291                  3   \n",
              "5   28    Teacher       34847.84         3037.986667                  2   \n",
              "8   35   Engineer      143162.64         4182.004291                  1   \n",
              "\n",
              "   Num_Credit_Card  Interest_Rate  Num_of_Loan  Delay_from_due_date  \\\n",
              "0                4              3            4                    3   \n",
              "1                4              3            4                    3   \n",
              "3                4              3            4                    4   \n",
              "5                4              6            1                    3   \n",
              "8                5              8            3                    8   \n",
              "\n",
              "   Num_of_Delayed_Payment  ...  Monthly_Balance  AutoLoan Credit-BuilderLoan  \\\n",
              "0                       7  ...       186.266702         1                  1   \n",
              "1                       9  ...       361.444004         1                  1   \n",
              "3                       5  ...       343.826873         1                  1   \n",
              "5                       3  ...       303.355083         0                  1   \n",
              "8                    1942  ...       854.226027         2                  0   \n",
              "\n",
              "   DebtConsolidationLoan  HomeEquityLoan MortgageLoan  NotSpecified  \\\n",
              "0                      0               1            0             0   \n",
              "1                      0               1            0             0   \n",
              "3                      0               1            0             0   \n",
              "5                      0               0            0             0   \n",
              "8                      0               0            0             1   \n",
              "\n",
              "   PaydayLoan PersonalLoan  StudentLoan  \n",
              "0           0            1            0  \n",
              "1           0            1            0  \n",
              "3           0            1            0  \n",
              "5           0            0            0  \n",
              "8           0            0            0  \n",
              "\n",
              "[5 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0820a68-aae5-429a-9fe4-5c11d2f878a8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Annual_Income</th>\n",
              "      <th>Monthly_Inh_Salary</th>\n",
              "      <th>Num_Bank_Accounts</th>\n",
              "      <th>Num_Credit_Card</th>\n",
              "      <th>Interest_Rate</th>\n",
              "      <th>Num_of_Loan</th>\n",
              "      <th>Delay_from_due_date</th>\n",
              "      <th>Num_of_Delayed_Payment</th>\n",
              "      <th>...</th>\n",
              "      <th>Monthly_Balance</th>\n",
              "      <th>AutoLoan</th>\n",
              "      <th>Credit-BuilderLoan</th>\n",
              "      <th>DebtConsolidationLoan</th>\n",
              "      <th>HomeEquityLoan</th>\n",
              "      <th>MortgageLoan</th>\n",
              "      <th>NotSpecified</th>\n",
              "      <th>PaydayLoan</th>\n",
              "      <th>PersonalLoan</th>\n",
              "      <th>StudentLoan</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23</td>\n",
              "      <td>Scientist</td>\n",
              "      <td>19114.12</td>\n",
              "      <td>1824.843333</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>186.266702</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>Scientist</td>\n",
              "      <td>19114.12</td>\n",
              "      <td>1824.843333</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>...</td>\n",
              "      <td>361.444004</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>Scientist</td>\n",
              "      <td>19114.12</td>\n",
              "      <td>4182.004291</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>343.826873</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>28</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>34847.84</td>\n",
              "      <td>3037.986667</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>303.355083</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>35</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>143162.64</td>\n",
              "      <td>4182.004291</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1942</td>\n",
              "      <td>...</td>\n",
              "      <td>854.226027</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 29 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0820a68-aae5-429a-9fe4-5c11d2f878a8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f0820a68-aae5-429a-9fe4-5c11d2f878a8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f0820a68-aae5-429a-9fe4-5c11d2f878a8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7a22a598-061f-4e3a-ac36-dc3f749d5c1e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a22a598-061f-4e3a-ac36-dc3f749d5c1e')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7a22a598-061f-4e3a-ac36-dc3f749d5c1e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "-vtwwrrOIH12",
        "outputId": "5c113d8b-af90-48e1-9480-7edc5a6b64c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Standard    13421\n",
              "Good         8963\n",
              "Bad          6839\n",
              "Name: Credit_Mix, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "df[\"Credit_Mix\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZENObtyefVk"
      },
      "source": [
        "# 1. Preparing the data\n",
        "## 1.1 Data cleaning\n",
        " Perform OHE over the \"Occupation\" feature\n",
        "\n",
        " Then, perform LE over Payment_of_Min_Amount and Payment_Behaviour\n",
        "\n",
        " _hint: As we will be testing only one model no need to define a pipeline_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "JGVrLNJTefVk"
      },
      "outputs": [],
      "source": [
        "occupation_dummies = pd.get_dummies(df['Occupation'], prefix='Occupation')\n",
        "df = pd.concat([df, occupation_dummies], axis=1)\n",
        "df.drop('Occupation', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "\n",
        "# Label Encoding for 'Payment_of_Min_Amount'\n",
        "df['Payment_of_Min_Amount'] = le.fit_transform(df['Payment_of_Min_Amount'])\n",
        "\n",
        "# Label Encoding for 'Payment_Behaviour'\n",
        "df['Payment_Behaviour'] = le.fit_transform(df['Payment_Behaviour'])"
      ],
      "metadata": {
        "id": "4A6YbZvXSNQv"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfPnhUxAefVl"
      },
      "source": [
        "## 1.2 Dataset splitting\n",
        "\n",
        "Split the dataset in two, first X with your independent features and then y with the dependent feature **CreditMix**.\n",
        "\n",
        "Then perform :\n",
        "* OneHotEncoding over the **CreditMix** feature.\n",
        "* A MinMaxScaller over the independent features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "J-mentarefVm"
      },
      "outputs": [],
      "source": [
        "# Your code here:\n",
        "X = df.drop('Credit_Mix', axis=1)\n",
        "y = df['Credit_Mix']\n",
        "\n",
        "\n",
        "\n",
        "#Define the scaler\n",
        "y_dummies = pd.get_dummies(y, prefix='Credit_Mix')\n",
        "# Define the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the scaler on independent features\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert the scaled data back to a dataframe\n",
        "X = pd.DataFrame(X_scaled, columns=X.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp07SutKIH16"
      },
      "source": [
        "### 1.2.1 Train Test splitting\n",
        "Now split the data in X_train, X_test, y_train, y_test,\n",
        "\n",
        "You can use test_size = 0.2 and a random_state of 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "yotEvoAxefVn"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y_dummies, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUZM-anTIH17"
      },
      "source": [
        "### 1.2.2 final touches\n",
        "Convert your datasets to `Torch tensors` of type `torch.float`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "2KS_U8stefVo",
        "outputId": "c37c0596-de6d-4c23-ad27-34a77d06de9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([23378, 42]) torch.Size([23378, 3])\n"
          ]
        }
      ],
      "source": [
        "i#Your code here:\n",
        "# Convert training data\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\n",
        "\n",
        "# Convert testing data\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n",
        "print(X_train_tensor.size(), y_train_tensor.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWY7E3HBIH17"
      },
      "source": [
        "# 2 Model preparation:\n",
        "\n",
        "## 2.1 Define a Neural network model and instantiate it.\n",
        "You can set the number of neurons to 150."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8qqRoocVefVp"
      },
      "outputs": [],
      "source": [
        "# Define a neural network class here:\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # Define layers\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.layer2(x)  # No activation function here since we're assuming a classification task.\n",
        "        return x\n",
        "\n",
        "# Define input, hidden, and output sizes\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 150\n",
        "output_size = y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "cx-3yvp5efVp",
        "outputId": "c2bc6dfe-9ebf-4771-df0d-96263728f0a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layer1): Linear(in_features=42, out_features=150, bias=True)\n",
            "  (layer2): Linear(in_features=150, out_features=3, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "6903\n"
          ]
        }
      ],
      "source": [
        "# Instantiate your model here\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "print(model)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQu3zXYPIH1_"
      },
      "source": [
        "## 2.2 finding the best model:\n",
        "Identify, amongst the following options the best parameters for your model:\n",
        "\n",
        "* `criterion` : [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), [BCEWithLogitsLoss](hhttps://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
        "* `iterations` : 150, 250, 500\n",
        "* `learning rate` : 0.00005, 0.001, 12.031\n",
        "\n",
        "\n",
        "_Hint: restart your runtime between each execution to ensure that previous neural networks dont interfere with your current one_\n",
        "\n",
        "_You can evaluate your model based on it's accuracy over the test set_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Xx9l8UOoefVq",
        "outputId": "afd7cbd4-752c-4546-b3f3-1f6cae4281b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Criterion: CrossEntropyLoss, Iterations: 150, Learning Rate: 5e-05, Accuracy: 0.4546\n",
            "Criterion: CrossEntropyLoss, Iterations: 150, Learning Rate: 0.001, Accuracy: 0.7755\n",
            "Criterion: CrossEntropyLoss, Iterations: 250, Learning Rate: 5e-05, Accuracy: 0.4556\n",
            "Criterion: CrossEntropyLoss, Iterations: 250, Learning Rate: 0.001, Accuracy: 0.8068\n",
            "Criterion: CrossEntropyLoss, Iterations: 500, Learning Rate: 5e-05, Accuracy: 0.5117\n",
            "Criterion: CrossEntropyLoss, Iterations: 500, Learning Rate: 0.001, Accuracy: 0.8393\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 150, Learning Rate: 5e-05, Accuracy: 0.6667\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 150, Learning Rate: 0.001, Accuracy: 0.8098\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 250, Learning Rate: 5e-05, Accuracy: 0.6667\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 250, Learning Rate: 0.001, Accuracy: 0.8429\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 500, Learning Rate: 5e-05, Accuracy: 0.6667\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 500, Learning Rate: 0.001, Accuracy: 0.8770\n",
            "\n",
            "Best Parameters:\n",
            "{'criterion': <class 'torch.nn.modules.loss.BCEWithLogitsLoss'>, 'iterations': 500, 'learning_rate': 0.001, 'accuracy': tensor(0.8770)}\n"
          ]
        }
      ],
      "source": [
        "# Placeholder for best parameters and their accuracy\n",
        "best_params = {\n",
        "    \"criterion\": None,\n",
        "    \"iterations\": None,\n",
        "    \"learning_rate\": None,\n",
        "    \"accuracy\": 0\n",
        "}\n",
        "\n",
        "# Potential hyperparameters to search over\n",
        "criteria = [nn.CrossEntropyLoss, nn.BCEWithLogitsLoss]\n",
        "iterations_list = [150, 250, 500]\n",
        "learning_rates = [0.00005, 0.001]\n",
        "\n",
        "for criterion in criteria:\n",
        "    for iters in iterations_list:\n",
        "        for lr in learning_rates:\n",
        "            # Instantiate the model\n",
        "            model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "            loss_function = criterion()\n",
        "\n",
        "            # Define Adam optimizer\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Training loop\n",
        "            for epoch in range(iters):\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(X_train_tensor)\n",
        "\n",
        "                if criterion == nn.CrossEntropyLoss:\n",
        "                    loss = loss_function(outputs, torch.max(y_train_tensor, 1)[1])\n",
        "                else:\n",
        "                    loss = loss_function(outputs, y_train_tensor)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Evaluate accuracy\n",
        "            with torch.no_grad():\n",
        "                predicted = model(X_test_tensor)\n",
        "                if criterion == nn.CrossEntropyLoss:\n",
        "                    _, predicted_labels = torch.max(predicted, 1)\n",
        "                    _, true_labels = torch.max(y_test_tensor, 1)\n",
        "                    accuracy = (predicted_labels == true_labels).float().mean()\n",
        "                else:\n",
        "                    accuracy = ((predicted > 0.5) == y_test_tensor).float().mean()\n",
        "\n",
        "            # Print performance for the current combination\n",
        "            print(f\"Criterion: {criterion.__name__}, Iterations: {iters}, Learning Rate: {lr}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            if accuracy > best_params[\"accuracy\"]:\n",
        "                best_params[\"criterion\"] = criterion\n",
        "                best_params[\"iterations\"] = iters\n",
        "                best_params[\"learning_rate\"] = lr\n",
        "                best_params[\"accuracy\"] = accuracy\n",
        "\n",
        "# Print the best parameters after all combinations\n",
        "print(\"\\nBest Parameters:\")\n",
        "print(best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Z_pmZ0yAefVr"
      },
      "outputs": [],
      "source": [
        "# Perform your iterations here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcFH_slCIH2A"
      },
      "source": [
        "## 2.3 Model Accuracy\n",
        "Identify the models accuracy over the train and test parts of the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "GDWG4JWaIH2A",
        "outputId": "31dad9c5-e31f-4885-b5d1-b9089810bbc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 88.69%\n",
            "Test Accuracy: 88.13%\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the best model based on the parameters\n",
        "best_model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "best_criterion = nn.BCEWithLogitsLoss()\n",
        "best_optimizer = torch.optim.Adam(best_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the best model\n",
        "for epoch in range(500):  # 500 iterations\n",
        "    best_optimizer.zero_grad()\n",
        "    outputs = best_model(X_train_tensor)\n",
        "    loss = best_criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    best_optimizer.step()\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(model, data, labels, criterion):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(data)\n",
        "        if criterion == nn.CrossEntropyLoss:\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            _, true_labels = torch.max(labels, 1)\n",
        "            accuracy = (predicted == true_labels).float().mean()\n",
        "        else:  # BCEWithLogitsLoss\n",
        "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
        "            accuracy = (predicted == labels).float().mean()\n",
        "    return accuracy.item()\n",
        "\n",
        "# Calculate training accuracy\n",
        "train_accuracy = calculate_accuracy(best_model, X_train_tensor, y_train_tensor, nn.BCEWithLogitsLoss)\n",
        "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = calculate_accuracy(best_model, X_test_tensor, y_test_tensor, nn.BCEWithLogitsLoss)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO0Fisg9IH2A"
      },
      "source": [
        "# 3. Predictions over the suspects dataset\n",
        "## 3.1 Retrain a new model over the full training dataset\n",
        "#### Please use the following parameters for this section:\n",
        "* ``neurons`` = 150\n",
        "* ``learning`` rate = 0.00005\n",
        "* ``criterion`` = CrossEntropyLoss\n",
        "* `iterations` = 500\n",
        "\n",
        "_hint you may have to redo some preprocessing as you did in part one_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "2BCgLIWJIH2B",
        "outputId": "c8758ee3-2529-4c0d-c3c2-82aae5a7cb15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 1.1010977029800415\n",
            "Epoch [2/500], Loss: 1.1007187366485596\n",
            "Epoch [3/500], Loss: 1.1003406047821045\n",
            "Epoch [4/500], Loss: 1.099963665008545\n",
            "Epoch [5/500], Loss: 1.0995874404907227\n",
            "Epoch [6/500], Loss: 1.099212408065796\n",
            "Epoch [7/500], Loss: 1.098838448524475\n",
            "Epoch [8/500], Loss: 1.0984654426574707\n",
            "Epoch [9/500], Loss: 1.0980935096740723\n",
            "Epoch [10/500], Loss: 1.0977226495742798\n",
            "Epoch [11/500], Loss: 1.0973528623580933\n",
            "Epoch [12/500], Loss: 1.0969840288162231\n",
            "Epoch [13/500], Loss: 1.096616506576538\n",
            "Epoch [14/500], Loss: 1.096250057220459\n",
            "Epoch [15/500], Loss: 1.0958845615386963\n",
            "Epoch [16/500], Loss: 1.0955201387405396\n",
            "Epoch [17/500], Loss: 1.0951569080352783\n",
            "Epoch [18/500], Loss: 1.0947948694229126\n",
            "Epoch [19/500], Loss: 1.0944339036941528\n",
            "Epoch [20/500], Loss: 1.0940741300582886\n",
            "Epoch [21/500], Loss: 1.0937151908874512\n",
            "Epoch [22/500], Loss: 1.0933575630187988\n",
            "Epoch [23/500], Loss: 1.093000888824463\n",
            "Epoch [24/500], Loss: 1.092645525932312\n",
            "Epoch [25/500], Loss: 1.092290997505188\n",
            "Epoch [26/500], Loss: 1.091937780380249\n",
            "Epoch [27/500], Loss: 1.0915855169296265\n",
            "Epoch [28/500], Loss: 1.0912344455718994\n",
            "Epoch [29/500], Loss: 1.0908843278884888\n",
            "Epoch [30/500], Loss: 1.0905355215072632\n",
            "Epoch [31/500], Loss: 1.090187668800354\n",
            "Epoch [32/500], Loss: 1.0898407697677612\n",
            "Epoch [33/500], Loss: 1.089495062828064\n",
            "Epoch [34/500], Loss: 1.089150309562683\n",
            "Epoch [35/500], Loss: 1.0888067483901978\n",
            "Epoch [36/500], Loss: 1.0884640216827393\n",
            "Epoch [37/500], Loss: 1.0881224870681763\n",
            "Epoch [38/500], Loss: 1.0877819061279297\n",
            "Epoch [39/500], Loss: 1.087442398071289\n",
            "Epoch [40/500], Loss: 1.0871038436889648\n",
            "Epoch [41/500], Loss: 1.086766242980957\n",
            "Epoch [42/500], Loss: 1.0864297151565552\n",
            "Epoch [43/500], Loss: 1.0860940217971802\n",
            "Epoch [44/500], Loss: 1.0857592821121216\n",
            "Epoch [45/500], Loss: 1.0854257345199585\n",
            "Epoch [46/500], Loss: 1.0850929021835327\n",
            "Epoch [47/500], Loss: 1.084761142730713\n",
            "Epoch [48/500], Loss: 1.08443021774292\n",
            "Epoch [49/500], Loss: 1.0841002464294434\n",
            "Epoch [50/500], Loss: 1.0837711095809937\n",
            "Epoch [51/500], Loss: 1.08344304561615\n",
            "Epoch [52/500], Loss: 1.083115816116333\n",
            "Epoch [53/500], Loss: 1.0827895402908325\n",
            "Epoch [54/500], Loss: 1.082464337348938\n",
            "Epoch [55/500], Loss: 1.0821397304534912\n",
            "Epoch [56/500], Loss: 1.0818160772323608\n",
            "Epoch [57/500], Loss: 1.0814933776855469\n",
            "Epoch [58/500], Loss: 1.0811713933944702\n",
            "Epoch [59/500], Loss: 1.08085036277771\n",
            "Epoch [60/500], Loss: 1.0805301666259766\n",
            "Epoch [61/500], Loss: 1.0802106857299805\n",
            "Epoch [62/500], Loss: 1.0798920392990112\n",
            "Epoch [63/500], Loss: 1.0795741081237793\n",
            "Epoch [64/500], Loss: 1.0792570114135742\n",
            "Epoch [65/500], Loss: 1.078940749168396\n",
            "Epoch [66/500], Loss: 1.0786253213882446\n",
            "Epoch [67/500], Loss: 1.0783106088638306\n",
            "Epoch [68/500], Loss: 1.0779964923858643\n",
            "Epoch [69/500], Loss: 1.0776833295822144\n",
            "Epoch [70/500], Loss: 1.0773710012435913\n",
            "Epoch [71/500], Loss: 1.077059268951416\n",
            "Epoch [72/500], Loss: 1.0767483711242676\n",
            "Epoch [73/500], Loss: 1.076438069343567\n",
            "Epoch [74/500], Loss: 1.0761287212371826\n",
            "Epoch [75/500], Loss: 1.075819969177246\n",
            "Epoch [76/500], Loss: 1.0755118131637573\n",
            "Epoch [77/500], Loss: 1.0752044916152954\n",
            "Epoch [78/500], Loss: 1.0748980045318604\n",
            "Epoch [79/500], Loss: 1.074591875076294\n",
            "Epoch [80/500], Loss: 1.074286699295044\n",
            "Epoch [81/500], Loss: 1.0739820003509521\n",
            "Epoch [82/500], Loss: 1.0736780166625977\n",
            "Epoch [83/500], Loss: 1.073374629020691\n",
            "Epoch [84/500], Loss: 1.0730719566345215\n",
            "Epoch [85/500], Loss: 1.0727698802947998\n",
            "Epoch [86/500], Loss: 1.0724685192108154\n",
            "Epoch [87/500], Loss: 1.0721676349639893\n",
            "Epoch [88/500], Loss: 1.0718673467636108\n",
            "Epoch [89/500], Loss: 1.0715678930282593\n",
            "Epoch [90/500], Loss: 1.071268916130066\n",
            "Epoch [91/500], Loss: 1.0709705352783203\n",
            "Epoch [92/500], Loss: 1.0706727504730225\n",
            "Epoch [93/500], Loss: 1.070375680923462\n",
            "Epoch [94/500], Loss: 1.07007896900177\n",
            "Epoch [95/500], Loss: 1.0697828531265259\n",
            "Epoch [96/500], Loss: 1.069487452507019\n",
            "Epoch [97/500], Loss: 1.0691925287246704\n",
            "Epoch [98/500], Loss: 1.0688982009887695\n",
            "Epoch [99/500], Loss: 1.0686042308807373\n",
            "Epoch [100/500], Loss: 1.0683109760284424\n",
            "Epoch [101/500], Loss: 1.0680180788040161\n",
            "Epoch [102/500], Loss: 1.0677257776260376\n",
            "Epoch [103/500], Loss: 1.0674340724945068\n",
            "Epoch [104/500], Loss: 1.0671426057815552\n",
            "Epoch [105/500], Loss: 1.0668517351150513\n",
            "Epoch [106/500], Loss: 1.0665613412857056\n",
            "Epoch [107/500], Loss: 1.066271424293518\n",
            "Epoch [108/500], Loss: 1.0659818649291992\n",
            "Epoch [109/500], Loss: 1.0656929016113281\n",
            "Epoch [110/500], Loss: 1.0654045343399048\n",
            "Epoch [111/500], Loss: 1.06511652469635\n",
            "Epoch [112/500], Loss: 1.064828872680664\n",
            "Epoch [113/500], Loss: 1.0645418167114258\n",
            "Epoch [114/500], Loss: 1.0642551183700562\n",
            "Epoch [115/500], Loss: 1.0639688968658447\n",
            "Epoch [116/500], Loss: 1.063683032989502\n",
            "Epoch [117/500], Loss: 1.0633976459503174\n",
            "Epoch [118/500], Loss: 1.0631126165390015\n",
            "Epoch [119/500], Loss: 1.0628283023834229\n",
            "Epoch [120/500], Loss: 1.0625439882278442\n",
            "Epoch [121/500], Loss: 1.062260389328003\n",
            "Epoch [122/500], Loss: 1.0619771480560303\n",
            "Epoch [123/500], Loss: 1.0616942644119263\n",
            "Epoch [124/500], Loss: 1.0614118576049805\n",
            "Epoch [125/500], Loss: 1.0611298084259033\n",
            "Epoch [126/500], Loss: 1.060848355293274\n",
            "Epoch [127/500], Loss: 1.0605671405792236\n",
            "Epoch [128/500], Loss: 1.060286283493042\n",
            "Epoch [129/500], Loss: 1.060006022453308\n",
            "Epoch [130/500], Loss: 1.0597258806228638\n",
            "Epoch [131/500], Loss: 1.0594462156295776\n",
            "Epoch [132/500], Loss: 1.0591669082641602\n",
            "Epoch [133/500], Loss: 1.0588878393173218\n",
            "Epoch [134/500], Loss: 1.0586092472076416\n",
            "Epoch [135/500], Loss: 1.0583308935165405\n",
            "Epoch [136/500], Loss: 1.0580530166625977\n",
            "Epoch [137/500], Loss: 1.0577754974365234\n",
            "Epoch [138/500], Loss: 1.0574980974197388\n",
            "Epoch [139/500], Loss: 1.0572210550308228\n",
            "Epoch [140/500], Loss: 1.0569446086883545\n",
            "Epoch [141/500], Loss: 1.0566681623458862\n",
            "Epoch [142/500], Loss: 1.0563921928405762\n",
            "Epoch [143/500], Loss: 1.0561163425445557\n",
            "Epoch [144/500], Loss: 1.0558409690856934\n",
            "Epoch [145/500], Loss: 1.055565595626831\n",
            "Epoch [146/500], Loss: 1.055290699005127\n",
            "Epoch [147/500], Loss: 1.055016040802002\n",
            "Epoch [148/500], Loss: 1.0547418594360352\n",
            "Epoch [149/500], Loss: 1.0544679164886475\n",
            "Epoch [150/500], Loss: 1.0541940927505493\n",
            "Epoch [151/500], Loss: 1.0539207458496094\n",
            "Epoch [152/500], Loss: 1.053647518157959\n",
            "Epoch [153/500], Loss: 1.0533745288848877\n",
            "Epoch [154/500], Loss: 1.053101897239685\n",
            "Epoch [155/500], Loss: 1.0528295040130615\n",
            "Epoch [156/500], Loss: 1.0525572299957275\n",
            "Epoch [157/500], Loss: 1.0522851943969727\n",
            "Epoch [158/500], Loss: 1.0520135164260864\n",
            "Epoch [159/500], Loss: 1.0517419576644897\n",
            "Epoch [160/500], Loss: 1.0514706373214722\n",
            "Epoch [161/500], Loss: 1.0511994361877441\n",
            "Epoch [162/500], Loss: 1.0509285926818848\n",
            "Epoch [163/500], Loss: 1.050657868385315\n",
            "Epoch [164/500], Loss: 1.0503873825073242\n",
            "Epoch [165/500], Loss: 1.0501172542572021\n",
            "Epoch [166/500], Loss: 1.0498472452163696\n",
            "Epoch [167/500], Loss: 1.0495774745941162\n",
            "Epoch [168/500], Loss: 1.049307942390442\n",
            "Epoch [169/500], Loss: 1.0490386486053467\n",
            "Epoch [170/500], Loss: 1.048769235610962\n",
            "Epoch [171/500], Loss: 1.0485001802444458\n",
            "Epoch [172/500], Loss: 1.0482312440872192\n",
            "Epoch [173/500], Loss: 1.0479624271392822\n",
            "Epoch [174/500], Loss: 1.0476936101913452\n",
            "Epoch [175/500], Loss: 1.0474251508712769\n",
            "Epoch [176/500], Loss: 1.0471569299697876\n",
            "Epoch [177/500], Loss: 1.0468885898590088\n",
            "Epoch [178/500], Loss: 1.046620488166809\n",
            "Epoch [179/500], Loss: 1.046352505683899\n",
            "Epoch [180/500], Loss: 1.0460845232009888\n",
            "Epoch [181/500], Loss: 1.0458170175552368\n",
            "Epoch [182/500], Loss: 1.0455492734909058\n",
            "Epoch [183/500], Loss: 1.045282006263733\n",
            "Epoch [184/500], Loss: 1.0450146198272705\n",
            "Epoch [185/500], Loss: 1.0447474718093872\n",
            "Epoch [186/500], Loss: 1.0444802045822144\n",
            "Epoch [187/500], Loss: 1.0442132949829102\n",
            "Epoch [188/500], Loss: 1.0439462661743164\n",
            "Epoch [189/500], Loss: 1.0436794757843018\n",
            "Epoch [190/500], Loss: 1.043412685394287\n",
            "Epoch [191/500], Loss: 1.0431461334228516\n",
            "Epoch [192/500], Loss: 1.042879581451416\n",
            "Epoch [193/500], Loss: 1.04261314868927\n",
            "Epoch [194/500], Loss: 1.0423468351364136\n",
            "Epoch [195/500], Loss: 1.0420805215835571\n",
            "Epoch [196/500], Loss: 1.0418144464492798\n",
            "Epoch [197/500], Loss: 1.041548252105713\n",
            "Epoch [198/500], Loss: 1.0412824153900146\n",
            "Epoch [199/500], Loss: 1.0410164594650269\n",
            "Epoch [200/500], Loss: 1.0407507419586182\n",
            "Epoch [201/500], Loss: 1.04048490524292\n",
            "Epoch [202/500], Loss: 1.0402190685272217\n",
            "Epoch [203/500], Loss: 1.039953351020813\n",
            "Epoch [204/500], Loss: 1.0396877527236938\n",
            "Epoch [205/500], Loss: 1.0394221544265747\n",
            "Epoch [206/500], Loss: 1.0391566753387451\n",
            "Epoch [207/500], Loss: 1.0388911962509155\n",
            "Epoch [208/500], Loss: 1.038625717163086\n",
            "Epoch [209/500], Loss: 1.0383601188659668\n",
            "Epoch [210/500], Loss: 1.0380947589874268\n",
            "Epoch [211/500], Loss: 1.0378291606903076\n",
            "Epoch [212/500], Loss: 1.0375635623931885\n",
            "Epoch [213/500], Loss: 1.0372982025146484\n",
            "Epoch [214/500], Loss: 1.0370327234268188\n",
            "Epoch [215/500], Loss: 1.0367673635482788\n",
            "Epoch [216/500], Loss: 1.0365017652511597\n",
            "Epoch [217/500], Loss: 1.03623628616333\n",
            "Epoch [218/500], Loss: 1.0359708070755005\n",
            "Epoch [219/500], Loss: 1.035705327987671\n",
            "Epoch [220/500], Loss: 1.0354398488998413\n",
            "Epoch [221/500], Loss: 1.0351742506027222\n",
            "Epoch [222/500], Loss: 1.034908652305603\n",
            "Epoch [223/500], Loss: 1.0346430540084839\n",
            "Epoch [224/500], Loss: 1.034377098083496\n",
            "Epoch [225/500], Loss: 1.0341113805770874\n",
            "Epoch [226/500], Loss: 1.0338455438613892\n",
            "Epoch [227/500], Loss: 1.033579707145691\n",
            "Epoch [228/500], Loss: 1.033313512802124\n",
            "Epoch [229/500], Loss: 1.0330474376678467\n",
            "Epoch [230/500], Loss: 1.0327813625335693\n",
            "Epoch [231/500], Loss: 1.0325151681900024\n",
            "Epoch [232/500], Loss: 1.032248854637146\n",
            "Epoch [233/500], Loss: 1.0319825410842896\n",
            "Epoch [234/500], Loss: 1.0317158699035645\n",
            "Epoch [235/500], Loss: 1.0314491987228394\n",
            "Epoch [236/500], Loss: 1.0311826467514038\n",
            "Epoch [237/500], Loss: 1.0309158563613892\n",
            "Epoch [238/500], Loss: 1.030648946762085\n",
            "Epoch [239/500], Loss: 1.0303817987442017\n",
            "Epoch [240/500], Loss: 1.0301144123077393\n",
            "Epoch [241/500], Loss: 1.0298469066619873\n",
            "Epoch [242/500], Loss: 1.0295792818069458\n",
            "Epoch [243/500], Loss: 1.0293114185333252\n",
            "Epoch [244/500], Loss: 1.0290435552597046\n",
            "Epoch [245/500], Loss: 1.0287753343582153\n",
            "Epoch [246/500], Loss: 1.0285069942474365\n",
            "Epoch [247/500], Loss: 1.0282384157180786\n",
            "Epoch [248/500], Loss: 1.0279695987701416\n",
            "Epoch [249/500], Loss: 1.0277007818222046\n",
            "Epoch [250/500], Loss: 1.027431607246399\n",
            "Epoch [251/500], Loss: 1.0271624326705933\n",
            "Epoch [252/500], Loss: 1.026893138885498\n",
            "Epoch [253/500], Loss: 1.0266233682632446\n",
            "Epoch [254/500], Loss: 1.0263535976409912\n",
            "Epoch [255/500], Loss: 1.0260835886001587\n",
            "Epoch [256/500], Loss: 1.025813341140747\n",
            "Epoch [257/500], Loss: 1.0255430936813354\n",
            "Epoch [258/500], Loss: 1.0252723693847656\n",
            "Epoch [259/500], Loss: 1.0250015258789062\n",
            "Epoch [260/500], Loss: 1.0247305631637573\n",
            "Epoch [261/500], Loss: 1.0244591236114502\n",
            "Epoch [262/500], Loss: 1.0241875648498535\n",
            "Epoch [263/500], Loss: 1.0239157676696777\n",
            "Epoch [264/500], Loss: 1.0236434936523438\n",
            "Epoch [265/500], Loss: 1.0233713388442993\n",
            "Epoch [266/500], Loss: 1.0230987071990967\n",
            "Epoch [267/500], Loss: 1.022825837135315\n",
            "Epoch [268/500], Loss: 1.022552728652954\n",
            "Epoch [269/500], Loss: 1.022279143333435\n",
            "Epoch [270/500], Loss: 1.022005319595337\n",
            "Epoch [271/500], Loss: 1.0217313766479492\n",
            "Epoch [272/500], Loss: 1.0214568376541138\n",
            "Epoch [273/500], Loss: 1.0211822986602783\n",
            "Epoch [274/500], Loss: 1.0209072828292847\n",
            "Epoch [275/500], Loss: 1.0206319093704224\n",
            "Epoch [276/500], Loss: 1.0203560590744019\n",
            "Epoch [277/500], Loss: 1.020080327987671\n",
            "Epoch [278/500], Loss: 1.0198040008544922\n",
            "Epoch [279/500], Loss: 1.0195274353027344\n",
            "Epoch [280/500], Loss: 1.0192506313323975\n",
            "Epoch [281/500], Loss: 1.0189733505249023\n",
            "Epoch [282/500], Loss: 1.0186960697174072\n",
            "Epoch [283/500], Loss: 1.0184181928634644\n",
            "Epoch [284/500], Loss: 1.0181400775909424\n",
            "Epoch [285/500], Loss: 1.0178616046905518\n",
            "Epoch [286/500], Loss: 1.017582654953003\n",
            "Epoch [287/500], Loss: 1.017303466796875\n",
            "Epoch [288/500], Loss: 1.0170239210128784\n",
            "Epoch [289/500], Loss: 1.0167438983917236\n",
            "Epoch [290/500], Loss: 1.0164636373519897\n",
            "Epoch [291/500], Loss: 1.0161830186843872\n",
            "Epoch [292/500], Loss: 1.0159021615982056\n",
            "Epoch [293/500], Loss: 1.0156209468841553\n",
            "Epoch [294/500], Loss: 1.0153392553329468\n",
            "Epoch [295/500], Loss: 1.0150574445724487\n",
            "Epoch [296/500], Loss: 1.0147751569747925\n",
            "Epoch [297/500], Loss: 1.014492392539978\n",
            "Epoch [298/500], Loss: 1.0142091512680054\n",
            "Epoch [299/500], Loss: 1.0139257907867432\n",
            "Epoch [300/500], Loss: 1.0136417150497437\n",
            "Epoch [301/500], Loss: 1.0133572816848755\n",
            "Epoch [302/500], Loss: 1.0130724906921387\n",
            "Epoch [303/500], Loss: 1.0127873420715332\n",
            "Epoch [304/500], Loss: 1.01250159740448\n",
            "Epoch [305/500], Loss: 1.0122156143188477\n",
            "Epoch [306/500], Loss: 1.0119290351867676\n",
            "Epoch [307/500], Loss: 1.0116420984268188\n",
            "Epoch [308/500], Loss: 1.011354684829712\n",
            "Epoch [309/500], Loss: 1.0110665559768677\n",
            "Epoch [310/500], Loss: 1.0107783079147339\n",
            "Epoch [311/500], Loss: 1.0104897022247314\n",
            "Epoch [312/500], Loss: 1.0102005004882812\n",
            "Epoch [313/500], Loss: 1.0099108219146729\n",
            "Epoch [314/500], Loss: 1.0096207857131958\n",
            "Epoch [315/500], Loss: 1.009330153465271\n",
            "Epoch [316/500], Loss: 1.0090391635894775\n",
            "Epoch [317/500], Loss: 1.0087475776672363\n",
            "Epoch [318/500], Loss: 1.0084556341171265\n",
            "Epoch [319/500], Loss: 1.0081632137298584\n",
            "Epoch [320/500], Loss: 1.0078704357147217\n",
            "Epoch [321/500], Loss: 1.0075769424438477\n",
            "Epoch [322/500], Loss: 1.007283329963684\n",
            "Epoch [323/500], Loss: 1.0069891214370728\n",
            "Epoch [324/500], Loss: 1.0066943168640137\n",
            "Epoch [325/500], Loss: 1.006399154663086\n",
            "Epoch [326/500], Loss: 1.006103515625\n",
            "Epoch [327/500], Loss: 1.0058072805404663\n",
            "Epoch [328/500], Loss: 1.0055105686187744\n",
            "Epoch [329/500], Loss: 1.0052136182785034\n",
            "Epoch [330/500], Loss: 1.0049158334732056\n",
            "Epoch [331/500], Loss: 1.004617691040039\n",
            "Epoch [332/500], Loss: 1.0043188333511353\n",
            "Epoch [333/500], Loss: 1.0040194988250732\n",
            "Epoch [334/500], Loss: 1.0037195682525635\n",
            "Epoch [335/500], Loss: 1.003419280052185\n",
            "Epoch [336/500], Loss: 1.0031182765960693\n",
            "Epoch [337/500], Loss: 1.002816915512085\n",
            "Epoch [338/500], Loss: 1.0025148391723633\n",
            "Epoch [339/500], Loss: 1.0022121667861938\n",
            "Epoch [340/500], Loss: 1.0019090175628662\n",
            "Epoch [341/500], Loss: 1.0016052722930908\n",
            "Epoch [342/500], Loss: 1.0013010501861572\n",
            "Epoch [343/500], Loss: 1.0009963512420654\n",
            "Epoch [344/500], Loss: 1.0006910562515259\n",
            "Epoch [345/500], Loss: 1.000385046005249\n",
            "Epoch [346/500], Loss: 1.000078797340393\n",
            "Epoch [347/500], Loss: 0.9997718930244446\n",
            "Epoch [348/500], Loss: 0.9994644522666931\n",
            "Epoch [349/500], Loss: 0.9991564154624939\n",
            "Epoch [350/500], Loss: 0.9988479018211365\n",
            "Epoch [351/500], Loss: 0.9985388517379761\n",
            "Epoch [352/500], Loss: 0.9982292652130127\n",
            "Epoch [353/500], Loss: 0.9979190230369568\n",
            "Epoch [354/500], Loss: 0.9976081848144531\n",
            "Epoch [355/500], Loss: 0.9972970485687256\n",
            "Epoch [356/500], Loss: 0.9969851970672607\n",
            "Epoch [357/500], Loss: 0.9966727495193481\n",
            "Epoch [358/500], Loss: 0.9963595867156982\n",
            "Epoch [359/500], Loss: 0.9960461854934692\n",
            "Epoch [360/500], Loss: 0.9957320690155029\n",
            "Epoch [361/500], Loss: 0.9954172968864441\n",
            "Epoch [362/500], Loss: 0.9951018691062927\n",
            "Epoch [363/500], Loss: 0.9947859048843384\n",
            "Epoch [364/500], Loss: 0.994469404220581\n",
            "Epoch [365/500], Loss: 0.9941521286964417\n",
            "Epoch [366/500], Loss: 0.9938346147537231\n",
            "Epoch [367/500], Loss: 0.9935162663459778\n",
            "Epoch [368/500], Loss: 0.9931973218917847\n",
            "Epoch [369/500], Loss: 0.9928776621818542\n",
            "Epoch [370/500], Loss: 0.9925574660301208\n",
            "Epoch [371/500], Loss: 0.9922367334365845\n",
            "Epoch [372/500], Loss: 0.9919151663780212\n",
            "Epoch [373/500], Loss: 0.991593062877655\n",
            "Epoch [374/500], Loss: 0.9912704825401306\n",
            "Epoch [375/500], Loss: 0.9909472465515137\n",
            "Epoch [376/500], Loss: 0.9906233549118042\n",
            "Epoch [377/500], Loss: 0.9902990460395813\n",
            "Epoch [378/500], Loss: 0.9899741411209106\n",
            "Epoch [379/500], Loss: 0.9896486401557922\n",
            "Epoch [380/500], Loss: 0.9893225431442261\n",
            "Epoch [381/500], Loss: 0.9889959096908569\n",
            "Epoch [382/500], Loss: 0.9886683821678162\n",
            "Epoch [383/500], Loss: 0.9883405566215515\n",
            "Epoch [384/500], Loss: 0.9880122542381287\n",
            "Epoch [385/500], Loss: 0.9876832365989685\n",
            "Epoch [386/500], Loss: 0.9873535633087158\n",
            "Epoch [387/500], Loss: 0.9870234727859497\n",
            "Epoch [388/500], Loss: 0.9866926074028015\n",
            "Epoch [389/500], Loss: 0.9863612651824951\n",
            "Epoch [390/500], Loss: 0.9860292077064514\n",
            "Epoch [391/500], Loss: 0.9856964945793152\n",
            "Epoch [392/500], Loss: 0.9853631854057312\n",
            "Epoch [393/500], Loss: 0.9850293397903442\n",
            "Epoch [394/500], Loss: 0.9846946597099304\n",
            "Epoch [395/500], Loss: 0.9843594431877136\n",
            "Epoch [396/500], Loss: 0.9840235114097595\n",
            "Epoch [397/500], Loss: 0.9836871027946472\n",
            "Epoch [398/500], Loss: 0.9833499193191528\n",
            "Epoch [399/500], Loss: 0.9830120205879211\n",
            "Epoch [400/500], Loss: 0.982673704624176\n",
            "Epoch [401/500], Loss: 0.9823347926139832\n",
            "Epoch [402/500], Loss: 0.9819954037666321\n",
            "Epoch [403/500], Loss: 0.9816552400588989\n",
            "Epoch [404/500], Loss: 0.9813145399093628\n",
            "Epoch [405/500], Loss: 0.9809731841087341\n",
            "Epoch [406/500], Loss: 0.980631411075592\n",
            "Epoch [407/500], Loss: 0.9802889823913574\n",
            "Epoch [408/500], Loss: 0.9799458980560303\n",
            "Epoch [409/500], Loss: 0.9796022176742554\n",
            "Epoch [410/500], Loss: 0.9792579412460327\n",
            "Epoch [411/500], Loss: 0.9789130687713623\n",
            "Epoch [412/500], Loss: 0.9785674810409546\n",
            "Epoch [413/500], Loss: 0.9782212972640991\n",
            "Epoch [414/500], Loss: 0.9778743386268616\n",
            "Epoch [415/500], Loss: 0.977526843547821\n",
            "Epoch [416/500], Loss: 0.9771787524223328\n",
            "Epoch [417/500], Loss: 0.976830005645752\n",
            "Epoch [418/500], Loss: 0.9764806032180786\n",
            "Epoch [419/500], Loss: 0.9761307239532471\n",
            "Epoch [420/500], Loss: 0.9757803678512573\n",
            "Epoch [421/500], Loss: 0.9754291772842407\n",
            "Epoch [422/500], Loss: 0.9750774502754211\n",
            "Epoch [423/500], Loss: 0.974725067615509\n",
            "Epoch [424/500], Loss: 0.9743720889091492\n",
            "Epoch [425/500], Loss: 0.9740185141563416\n",
            "Epoch [426/500], Loss: 0.9736642837524414\n",
            "Epoch [427/500], Loss: 0.9733095765113831\n",
            "Epoch [428/500], Loss: 0.9729541540145874\n",
            "Epoch [429/500], Loss: 0.9725980162620544\n",
            "Epoch [430/500], Loss: 0.9722413420677185\n",
            "Epoch [431/500], Loss: 0.97188401222229\n",
            "Epoch [432/500], Loss: 0.971526026725769\n",
            "Epoch [433/500], Loss: 0.9711673259735107\n",
            "Epoch [434/500], Loss: 0.9708081483840942\n",
            "Epoch [435/500], Loss: 0.9704483151435852\n",
            "Epoch [436/500], Loss: 0.9700878262519836\n",
            "Epoch [437/500], Loss: 0.9697265028953552\n",
            "Epoch [438/500], Loss: 0.969364583492279\n",
            "Epoch [439/500], Loss: 0.9690020680427551\n",
            "Epoch [440/500], Loss: 0.9686389565467834\n",
            "Epoch [441/500], Loss: 0.9682751893997192\n",
            "Epoch [442/500], Loss: 0.967910885810852\n",
            "Epoch [443/500], Loss: 0.9675460457801819\n",
            "Epoch [444/500], Loss: 0.9671804904937744\n",
            "Epoch [445/500], Loss: 0.9668141007423401\n",
            "Epoch [446/500], Loss: 0.9664471745491028\n",
            "Epoch [447/500], Loss: 0.9660795331001282\n",
            "Epoch [448/500], Loss: 0.9657111763954163\n",
            "Epoch [449/500], Loss: 0.965342104434967\n",
            "Epoch [450/500], Loss: 0.9649723172187805\n",
            "Epoch [451/500], Loss: 0.9646018147468567\n",
            "Epoch [452/500], Loss: 0.9642305374145508\n",
            "Epoch [453/500], Loss: 0.9638587236404419\n",
            "Epoch [454/500], Loss: 0.9634863138198853\n",
            "Epoch [455/500], Loss: 0.9631131291389465\n",
            "Epoch [456/500], Loss: 0.9627392292022705\n",
            "Epoch [457/500], Loss: 0.9623647928237915\n",
            "Epoch [458/500], Loss: 0.9619895219802856\n",
            "Epoch [459/500], Loss: 0.9616135358810425\n",
            "Epoch [460/500], Loss: 0.961236834526062\n",
            "Epoch [461/500], Loss: 0.9608595371246338\n",
            "Epoch [462/500], Loss: 0.9604817032814026\n",
            "Epoch [463/500], Loss: 0.9601030349731445\n",
            "Epoch [464/500], Loss: 0.9597238898277283\n",
            "Epoch [465/500], Loss: 0.9593442678451538\n",
            "Epoch [466/500], Loss: 0.9589638113975525\n",
            "Epoch [467/500], Loss: 0.9585829377174377\n",
            "Epoch [468/500], Loss: 0.9582012295722961\n",
            "Epoch [469/500], Loss: 0.9578188061714172\n",
            "Epoch [470/500], Loss: 0.9574358463287354\n",
            "Epoch [471/500], Loss: 0.9570522308349609\n",
            "Epoch [472/500], Loss: 0.9566680788993835\n",
            "Epoch [473/500], Loss: 0.9562830924987793\n",
            "Epoch [474/500], Loss: 0.9558973908424377\n",
            "Epoch [475/500], Loss: 0.9555113911628723\n",
            "Epoch [476/500], Loss: 0.9551246762275696\n",
            "Epoch [477/500], Loss: 0.9547372460365295\n",
            "Epoch [478/500], Loss: 0.9543492794036865\n",
            "Epoch [479/500], Loss: 0.9539607167243958\n",
            "Epoch [480/500], Loss: 0.9535714387893677\n",
            "Epoch [481/500], Loss: 0.9531816244125366\n",
            "Epoch [482/500], Loss: 0.952791154384613\n",
            "Epoch [483/500], Loss: 0.952400267124176\n",
            "Epoch [484/500], Loss: 0.9520084857940674\n",
            "Epoch [485/500], Loss: 0.9516163468360901\n",
            "Epoch [486/500], Loss: 0.9512236714363098\n",
            "Epoch [487/500], Loss: 0.9508303999900818\n",
            "Epoch [488/500], Loss: 0.9504368305206299\n",
            "Epoch [489/500], Loss: 0.9500426054000854\n",
            "Epoch [490/500], Loss: 0.9496477842330933\n",
            "Epoch [491/500], Loss: 0.9492522478103638\n",
            "Epoch [492/500], Loss: 0.9488562345504761\n",
            "Epoch [493/500], Loss: 0.948459804058075\n",
            "Epoch [494/500], Loss: 0.948062539100647\n",
            "Epoch [495/500], Loss: 0.9476646184921265\n",
            "Epoch [496/500], Loss: 0.9472663998603821\n",
            "Epoch [497/500], Loss: 0.9468674659729004\n",
            "Epoch [498/500], Loss: 0.946467936038971\n",
            "Epoch [499/500], Loss: 0.9460678100585938\n",
            "Epoch [500/500], Loss: 0.9456673264503479\n"
          ]
        }
      ],
      "source": [
        "# Define a new model here:\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 150)\n",
        "        self.layer2 = nn.Linear(150, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "model = NeuralNetwork()\n",
        "# Define your MSE loss here:\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define your Adam optimizer for finding the weights of the network here:\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
        "\n",
        "# perform your training here\n",
        "for epoch in range(500):\n",
        "\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/500], Loss: {loss.item()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Ff05w-MEIH2B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "eiYsXO8zIH2B"
      },
      "outputs": [],
      "source": [
        "# perform your training here"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B59vLG3cVQSy"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nynyX0sGVd7a"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FA6ofKplVVMf"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ccEPPGdIH2C"
      },
      "source": [
        "## 3.2 Predict over the suspects dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "OhZ1Ugv2IH2C",
        "outputId": "4501e424-fd0a-4f21-bfa5-3b3b71127b62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[317991,\n",
              " 241892,\n",
              " 303376,\n",
              " 761992,\n",
              " 373318,\n",
              " 938770,\n",
              " 676003,\n",
              " 255073,\n",
              " 468560,\n",
              " 263592,\n",
              " 660506,\n",
              " 244162,\n",
              " 774730,\n",
              " 911014,\n",
              " 601633,\n",
              " 645054,\n",
              " 255830,\n",
              " 353527,\n",
              " 531937,\n",
              " 413250,\n",
              " 601958,\n",
              " 596063,\n",
              " 764821,\n",
              " 880689,\n",
              " 340001,\n",
              " 512604,\n",
              " 165305,\n",
              " 334769,\n",
              " 564061,\n",
              " 306495,\n",
              " 647912,\n",
              " 946059,\n",
              " 917315,\n",
              " 504629,\n",
              " 520229,\n",
              " 861915,\n",
              " 308845,\n",
              " 654987,\n",
              " 398531,\n",
              " 327507,\n",
              " 888670,\n",
              " 449514,\n",
              " 819045,\n",
              " 899768,\n",
              " 251233,\n",
              " 344946,\n",
              " 932285,\n",
              " 169664,\n",
              " 265615,\n",
              " 867078,\n",
              " 618568,\n",
              " 512128,\n",
              " 222875,\n",
              " 160212,\n",
              " 466624,\n",
              " 903438,\n",
              " 957997,\n",
              " 362662,\n",
              " 892546,\n",
              " 777470,\n",
              " 397912,\n",
              " 123562,\n",
              " 939524,\n",
              " 387676,\n",
              " 860681,\n",
              " 789726,\n",
              " 167822,\n",
              " 571370,\n",
              " 863210,\n",
              " 717780,\n",
              " 863592,\n",
              " 898254,\n",
              " 424603,\n",
              " 632973,\n",
              " 599462,\n",
              " 601265,\n",
              " 402663,\n",
              " 226609,\n",
              " 790344,\n",
              " 862416,\n",
              " 134514,\n",
              " 177842,\n",
              " 551506,\n",
              " 365406,\n",
              " 825733,\n",
              " 220420,\n",
              " 927140,\n",
              " 596661,\n",
              " 688784,\n",
              " 829946,\n",
              " 242912,\n",
              " 443524,\n",
              " 464891,\n",
              " 380835,\n",
              " 823355,\n",
              " 456546,\n",
              " 790046,\n",
              " 932281,\n",
              " 538431,\n",
              " 297653,\n",
              " 206269,\n",
              " 96336,\n",
              " 273453,\n",
              " 112768,\n",
              " 446376,\n",
              " 205381,\n",
              " 614048,\n",
              " 931353,\n",
              " 773737,\n",
              " 793505,\n",
              " 419563,\n",
              " 299485,\n",
              " 752845,\n",
              " 278360,\n",
              " 494514,\n",
              " 571364,\n",
              " 303529,\n",
              " 796491,\n",
              " 205775,\n",
              " 133154,\n",
              " 182040,\n",
              " 425341,\n",
              " 778887,\n",
              " 583984,\n",
              " 539227,\n",
              " 676494,\n",
              " 430864,\n",
              " 899581,\n",
              " 240613,\n",
              " 859112,\n",
              " 541833,\n",
              " 126115,\n",
              " 839591,\n",
              " 453712,\n",
              " 544861,\n",
              " 544078,\n",
              " 183509,\n",
              " 259083,\n",
              " 512432,\n",
              " 874150,\n",
              " 484182,\n",
              " 745938,\n",
              " 291359,\n",
              " 499672,\n",
              " 757865,\n",
              " 744124,\n",
              " 641947,\n",
              " 631749,\n",
              " 306277,\n",
              " 235799,\n",
              " 710764,\n",
              " 781819,\n",
              " 666304,\n",
              " 936933,\n",
              " 553754,\n",
              " 549118,\n",
              " 700708,\n",
              " 817857,\n",
              " 874839,\n",
              " 693763,\n",
              " 367673,\n",
              " 619595,\n",
              " 103928,\n",
              " 245167,\n",
              " 893389,\n",
              " 397601,\n",
              " 442794,\n",
              " 949985,\n",
              " 422134,\n",
              " 852073,\n",
              " 822072,\n",
              " 628503,\n",
              " 953342,\n",
              " 397921,\n",
              " 927269,\n",
              " 228819,\n",
              " 913560,\n",
              " 648234,\n",
              " 294336,\n",
              " 943792,\n",
              " 267733,\n",
              " 638192,\n",
              " 353596,\n",
              " 350035,\n",
              " 570734,\n",
              " 743377,\n",
              " 605766,\n",
              " 957079,\n",
              " 150642,\n",
              " 356865,\n",
              " 621691,\n",
              " 888216,\n",
              " 210667,\n",
              " 794594,\n",
              " 524465,\n",
              " 740078,\n",
              " 754702,\n",
              " 429697,\n",
              " 741709,\n",
              " 510420,\n",
              " 204180,\n",
              " 564884,\n",
              " 412064,\n",
              " 490146,\n",
              " 881695,\n",
              " 933156,\n",
              " 107841,\n",
              " 416507,\n",
              " 441791,\n",
              " 565225,\n",
              " 303568,\n",
              " 304619,\n",
              " 866911,\n",
              " 168991,\n",
              " 649442,\n",
              " 754989,\n",
              " 937535,\n",
              " 376743,\n",
              " 767527,\n",
              " 225178,\n",
              " 519735,\n",
              " 953642,\n",
              " 125746,\n",
              " 623880,\n",
              " 720728,\n",
              " 836925,\n",
              " 449172,\n",
              " 198680,\n",
              " 428637,\n",
              " 702124,\n",
              " 706286,\n",
              " 934741,\n",
              " 792996,\n",
              " 391296,\n",
              " 318978,\n",
              " 149578,\n",
              " 156341,\n",
              " 140991,\n",
              " 641389,\n",
              " 421513,\n",
              " 460980,\n",
              " 184231,\n",
              " 355972,\n",
              " 806187,\n",
              " 628233,\n",
              " 522966,\n",
              " 925217,\n",
              " 535048,\n",
              " 168728,\n",
              " 241404,\n",
              " 703326,\n",
              " 325946,\n",
              " 154013,\n",
              " 448665,\n",
              " 814931,\n",
              " 537543,\n",
              " 803332,\n",
              " 581364,\n",
              " 455352,\n",
              " 128462,\n",
              " 151092,\n",
              " 875439,\n",
              " 418245,\n",
              " 843485,\n",
              " 99489,\n",
              " 231948,\n",
              " 645264,\n",
              " 649131,\n",
              " 382851,\n",
              " 108420,\n",
              " 339524,\n",
              " 509130,\n",
              " 719655,\n",
              " 316652,\n",
              " 653321,\n",
              " 569590,\n",
              " 517645,\n",
              " 481578,\n",
              " 438895,\n",
              " 942442,\n",
              " 304526,\n",
              " 393146,\n",
              " 434361,\n",
              " 548974,\n",
              " 726678,\n",
              " 804662,\n",
              " 456436,\n",
              " 661944,\n",
              " 108215,\n",
              " 378721,\n",
              " 573810,\n",
              " 769196,\n",
              " 804201,\n",
              " 395115,\n",
              " 287607,\n",
              " 866984,\n",
              " 791891,\n",
              " 631142,\n",
              " 780084,\n",
              " 708168,\n",
              " 131191,\n",
              " 474811,\n",
              " 490705,\n",
              " 898063,\n",
              " 409012,\n",
              " 610905,\n",
              " 621836,\n",
              " 387404,\n",
              " 454379,\n",
              " 503392,\n",
              " 859745,\n",
              " 931183,\n",
              " 780199,\n",
              " 651597,\n",
              " 411537,\n",
              " 795804,\n",
              " 431634,\n",
              " 883252,\n",
              " 857627,\n",
              " 795805,\n",
              " 372166,\n",
              " 581173,\n",
              " 98459,\n",
              " 280659,\n",
              " 536355,\n",
              " 431378,\n",
              " 935949,\n",
              " 201125,\n",
              " 304338,\n",
              " 704873,\n",
              " 131824,\n",
              " 204834,\n",
              " 172656,\n",
              " 527013,\n",
              " 424791,\n",
              " 465159,\n",
              " 132322,\n",
              " 861181,\n",
              " 455736,\n",
              " 422474,\n",
              " 411464,\n",
              " 415269,\n",
              " 119792,\n",
              " 788180,\n",
              " 713550,\n",
              " 347349,\n",
              " 427246,\n",
              " 589319,\n",
              " 509348,\n",
              " 383876,\n",
              " 439819,\n",
              " 887417,\n",
              " 258345,\n",
              " 792062,\n",
              " 351472,\n",
              " 416130,\n",
              " 694183,\n",
              " 231770,\n",
              " 679196,\n",
              " 425350,\n",
              " 544592,\n",
              " 550287,\n",
              " 729575,\n",
              " 813808,\n",
              " 597612,\n",
              " 826577,\n",
              " 951822,\n",
              " 352848,\n",
              " 219849,\n",
              " 183438,\n",
              " 339793,\n",
              " 131393,\n",
              " 410319,\n",
              " 793674,\n",
              " 386917,\n",
              " 200688,\n",
              " 382237,\n",
              " 224422,\n",
              " 772645,\n",
              " 169705,\n",
              " 638950,\n",
              " 698563,\n",
              " 949973,\n",
              " 643332,\n",
              " 486395,\n",
              " 915055,\n",
              " 505876,\n",
              " 673325,\n",
              " 223968,\n",
              " 96722,\n",
              " 902629,\n",
              " 531904,\n",
              " 128444,\n",
              " 233577,\n",
              " 123049,\n",
              " 437181,\n",
              " 628925,\n",
              " 376598,\n",
              " 919979,\n",
              " 897796,\n",
              " 907542,\n",
              " 571155,\n",
              " 257731,\n",
              " 213269,\n",
              " 269302,\n",
              " 623217,\n",
              " 789033,\n",
              " 601927,\n",
              " 674497,\n",
              " 474746,\n",
              " 739432,\n",
              " 793023,\n",
              " 724380,\n",
              " 682811,\n",
              " 520750,\n",
              " 637567,\n",
              " 806987,\n",
              " 128570,\n",
              " 945010,\n",
              " 748037,\n",
              " 867548,\n",
              " 453185,\n",
              " 586186,\n",
              " 886132,\n",
              " 362504,\n",
              " 115712,\n",
              " 410221,\n",
              " 750955,\n",
              " 253352,\n",
              " 947685,\n",
              " 802042,\n",
              " 952903,\n",
              " 948037,\n",
              " 569177,\n",
              " 404377,\n",
              " 947658,\n",
              " 96840,\n",
              " 131489,\n",
              " 917041,\n",
              " 739909,\n",
              " 284349,\n",
              " 188061,\n",
              " 784524,\n",
              " 354752,\n",
              " 325666,\n",
              " 223665,\n",
              " 846622,\n",
              " 285653,\n",
              " 543239,\n",
              " 623888,\n",
              " 926004,\n",
              " 553784,\n",
              " 485010,\n",
              " 648842,\n",
              " 556033,\n",
              " 931749,\n",
              " 334386,\n",
              " 843464,\n",
              " 333546,\n",
              " 242361,\n",
              " 333546,\n",
              " 802773,\n",
              " 444482,\n",
              " 139463,\n",
              " 833402,\n",
              " 102671,\n",
              " 711683,\n",
              " 843660,\n",
              " 501454,\n",
              " 466702,\n",
              " 208948,\n",
              " 587706,\n",
              " 918924,\n",
              " 245949,\n",
              " 803882,\n",
              " 623440,\n",
              " 405604,\n",
              " 138796,\n",
              " 739300,\n",
              " 232096,\n",
              " 765508,\n",
              " 872993,\n",
              " 190042,\n",
              " 119767,\n",
              " 655000,\n",
              " 332018,\n",
              " 515957,\n",
              " 809385,\n",
              " 403139,\n",
              " 261521,\n",
              " 245757,\n",
              " 267120,\n",
              " 428573,\n",
              " 284274,\n",
              " 949262,\n",
              " 520948,\n",
              " 208153,\n",
              " 136015,\n",
              " 773146,\n",
              " 820116,\n",
              " 410444,\n",
              " 128800,\n",
              " 327047,\n",
              " 413270,\n",
              " 790265,\n",
              " 152304,\n",
              " 572046,\n",
              " 651176,\n",
              " 418505,\n",
              " 476861,\n",
              " 906609,\n",
              " 874746,\n",
              " 354164,\n",
              " 837811,\n",
              " 818249,\n",
              " 371132,\n",
              " 702840,\n",
              " 654139,\n",
              " 123006,\n",
              " 344301,\n",
              " 299520,\n",
              " 165762,\n",
              " 742635,\n",
              " 345709,\n",
              " 812389,\n",
              " 564122,\n",
              " 409943,\n",
              " 426674,\n",
              " 601614,\n",
              " 119796,\n",
              " 649811,\n",
              " 858155,\n",
              " 429246,\n",
              " 329120,\n",
              " 722924,\n",
              " 405301,\n",
              " 315644,\n",
              " 640117,\n",
              " 884354,\n",
              " 792366,\n",
              " 842928,\n",
              " 605032,\n",
              " 317366,\n",
              " 221265,\n",
              " 128768,\n",
              " 139646,\n",
              " 446160,\n",
              " 325262,\n",
              " 277417,\n",
              " 257826,\n",
              " 407360,\n",
              " 579515,\n",
              " 769520,\n",
              " 808816,\n",
              " 628854,\n",
              " 427874,\n",
              " 110215,\n",
              " 592588,\n",
              " 856347,\n",
              " 327000,\n",
              " 858436,\n",
              " 878358,\n",
              " 195267,\n",
              " 213477,\n",
              " 574304,\n",
              " 692542,\n",
              " 910232,\n",
              " 359736,\n",
              " 790781,\n",
              " 328573,\n",
              " 586536,\n",
              " 249197,\n",
              " 166980,\n",
              " 269018,\n",
              " 726073,\n",
              " 483324,\n",
              " 442127,\n",
              " 505436,\n",
              " 929178,\n",
              " 601920,\n",
              " 481643,\n",
              " 347454,\n",
              " 634658,\n",
              " 444557,\n",
              " 919258,\n",
              " 932606,\n",
              " 299033,\n",
              " 156531,\n",
              " 109683,\n",
              " 545263,\n",
              " 823828,\n",
              " 464219,\n",
              " 316450,\n",
              " 685615,\n",
              " 957161,\n",
              " 762279,\n",
              " 296491,\n",
              " 441870,\n",
              " 649162,\n",
              " 662472,\n",
              " 681209,\n",
              " 441742,\n",
              " 403993,\n",
              " 364673,\n",
              " 238897,\n",
              " 398434,\n",
              " 852328,\n",
              " 401818,\n",
              " 105236,\n",
              " 638911,\n",
              " 716230,\n",
              " 267286,\n",
              " 411476,\n",
              " 453577,\n",
              " 206644,\n",
              " 952376,\n",
              " 717160,\n",
              " 178451,\n",
              " 325225,\n",
              " 251440,\n",
              " 785994,\n",
              " 783029,\n",
              " 880137,\n",
              " 136838,\n",
              " 191622,\n",
              " 642133,\n",
              " 156304,\n",
              " 910568,\n",
              " 776425,\n",
              " 804047,\n",
              " 788206,\n",
              " 820364,\n",
              " 819111,\n",
              " 526987,\n",
              " 851347,\n",
              " 487236,\n",
              " 708256,\n",
              " 613408,\n",
              " 905706,\n",
              " 104986,\n",
              " 223964,\n",
              " 887875,\n",
              " 428951,\n",
              " 96249,\n",
              " 236907,\n",
              " 649233,\n",
              " 191797,\n",
              " 268218,\n",
              " 237346,\n",
              " 516165,\n",
              " 447311,\n",
              " 302149,\n",
              " 738395,\n",
              " 582281,\n",
              " 719359,\n",
              " 230078,\n",
              " 858566,\n",
              " 745000,\n",
              " 739104,\n",
              " 263704,\n",
              " 185334,\n",
              " 828218,\n",
              " 585467,\n",
              " 401509,\n",
              " 457218,\n",
              " 241540,\n",
              " 790654,\n",
              " 528453,\n",
              " 852352,\n",
              " 599095,\n",
              " 353857,\n",
              " 213258,\n",
              " 408689,\n",
              " 587678,\n",
              " 687871,\n",
              " 297157,\n",
              " 123699,\n",
              " 581969,\n",
              " 414259,\n",
              " 123514,\n",
              " 895839,\n",
              " 934535,\n",
              " 409055,\n",
              " 850103,\n",
              " 162287,\n",
              " 707686,\n",
              " 942571,\n",
              " 907535,\n",
              " 832011,\n",
              " 539529,\n",
              " 680273,\n",
              " 247617,\n",
              " 543267,\n",
              " 868229,\n",
              " 789469,\n",
              " 389314,\n",
              " 620807,\n",
              " 248876,\n",
              " 866017,\n",
              " 887305,\n",
              " 459020,\n",
              " 911593,\n",
              " 682880,\n",
              " 300800,\n",
              " 571277,\n",
              " 281076,\n",
              " 458293,\n",
              " 218415,\n",
              " 173906,\n",
              " 178685,\n",
              " 200865]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "# Predict which users have a good credit score here:\n",
        "suspect_ids = suspects['userID']\n",
        "suspects_for_prediction = suspects.drop(columns=['userID'])\n",
        "suspects_tensor = torch.FloatTensor(suspects_for_prediction.values)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(suspects_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "# Convert predictions to a list or array\n",
        "predicted_list = predicted.numpy()\n",
        "\n",
        "# Create a DataFrame to map 'suspectID' to predictions\n",
        "results_df = pd.DataFrame({\n",
        "    'suspectID': suspect_ids,\n",
        "    'Prediction': predicted_list\n",
        "})\n",
        "\n",
        "# Identify which users have a good credit score\n",
        "good_credit_score_users = results_df[results_df['Prediction'] == 1]['suspectID'].tolist()\n",
        "good_credit_score_users"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}