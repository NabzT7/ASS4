{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLHkFsJkkTt4"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataScience_and_MachineLearning/blob/master/Assignements/Part%204/Assignment_part_four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZu-7QbP9muh"
      },
      "source": [
        "DSML investigation:\n",
        "\n",
        "You are part of the Suisse Impossible Mission Force, or SIMF for short. You need to uncover a rogue agent that is trying to steal sensitive information.\n",
        "\n",
        "Your mission, should you choose to accept it, is to find that agent before stealing any classified information. Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyL7WNdV9sWV"
      },
      "source": [
        "# Assignement part four\n",
        "### Due 22.10\n",
        "#### Identifying the suspects credit score\n",
        "We received informations that the rogue agent has a good credit score.\n",
        "\n",
        "Our spies at SIMF have managed to collect financial information relating to our suspects as well as a training dataset.\n",
        "\n",
        "Create a Neural Network over the training dataset `df` to identify which of the suspects have a good Credit_Mix\n",
        "\n",
        "\n",
        "## Getting to know our data\n",
        "\n",
        "* Age: a users age\n",
        "* Occupation: a users employment field\n",
        "* Annual_Income: a users annual income\n",
        "* Monthly_Inh_Salary: the calculated salary received by a given user on a monthly basis\n",
        "* Num_Bank_Accounts: the number of bank accounts possessed by a given user\n",
        "* Num_Credit_Cards: the number of credit card given user possesses\n",
        "* Interest_Rate: The interest rate on those cards (if multiple then its the average)\n",
        "* Num_of_Loans: The number of loans of each user\n",
        "* Delay_from_due_date: payment tardiness of user\n",
        "* Num_of_Delayed_Payment: the count of delayed payments\n",
        "* Changed_Credit_Limit: NaN\n",
        "* Num_Credit_Inquiries: NaN\n",
        "* Credit_Mix: The users credit score\n",
        "* Outsting_Debt: Outstanding debt\n",
        "* Credit_Utilization_Ratio: the percentage of borrowed money over borrowing allowance\n",
        "* Payment_of_Min_Amount: does the user usually pay the minimal amount (categorical)\n",
        "* Total_EMI_per_month: Monthly repayments to be made\n",
        "* Amount_invested_monthly: The amout put in an investment fun by the user on a monthly basis\n",
        "* Payment_Behaviour: the users payment behavior (categorical)\n",
        "* Monthly_Balance: The users end of the month balance\n",
        "* AutoLoan: If the user has an active loan for their vehicule\n",
        "* Credit-BuilderLoan: If the user has a loan to increase their credit score\n",
        "* DebtConsolidationLoan, HomeEquityLoan, MortgageLoan, NotSpecified, PaydayLoan, PersonalLoan, StudentLoan: different types of loans(categorical features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XHhI95r5-tyD"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1TF0jvYLkTuB"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/train_classification.csv\", index_col='Unnamed: 0').dropna()\n",
        "suspects = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Assignements/Part%204/data/suspects.csv\", index_col='Unnamed: 0').dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bhf4sBcKkTuC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "6cac43a4-3571-44e9-ed95-cd8c3f3be36e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Age Occupation  Annual_Income  Monthly_Inh_Salary  Num_Bank_Accounts  \\\n",
              "0   23  Scientist       19114.12         1824.843333                  3   \n",
              "1   24  Scientist       19114.12         1824.843333                  3   \n",
              "3   24  Scientist       19114.12         4182.004291                  3   \n",
              "5   28    Teacher       34847.84         3037.986667                  2   \n",
              "8   35   Engineer      143162.64         4182.004291                  1   \n",
              "\n",
              "   Num_Credit_Card  Interest_Rate  Num_of_Loan  Delay_from_due_date  \\\n",
              "0                4              3            4                    3   \n",
              "1                4              3            4                    3   \n",
              "3                4              3            4                    4   \n",
              "5                4              6            1                    3   \n",
              "8                5              8            3                    8   \n",
              "\n",
              "   Num_of_Delayed_Payment  ...  Monthly_Balance  AutoLoan Credit-BuilderLoan  \\\n",
              "0                       7  ...       186.266702         1                  1   \n",
              "1                       9  ...       361.444004         1                  1   \n",
              "3                       5  ...       343.826873         1                  1   \n",
              "5                       3  ...       303.355083         0                  1   \n",
              "8                    1942  ...       854.226027         2                  0   \n",
              "\n",
              "   DebtConsolidationLoan  HomeEquityLoan MortgageLoan  NotSpecified  \\\n",
              "0                      0               1            0             0   \n",
              "1                      0               1            0             0   \n",
              "3                      0               1            0             0   \n",
              "5                      0               0            0             0   \n",
              "8                      0               0            0             1   \n",
              "\n",
              "   PaydayLoan PersonalLoan  StudentLoan  \n",
              "0           0            1            0  \n",
              "1           0            1            0  \n",
              "3           0            1            0  \n",
              "5           0            0            0  \n",
              "8           0            0            0  \n",
              "\n",
              "[5 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99aca5b0-47a5-4fb0-ae91-14dea9adcb7c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Annual_Income</th>\n",
              "      <th>Monthly_Inh_Salary</th>\n",
              "      <th>Num_Bank_Accounts</th>\n",
              "      <th>Num_Credit_Card</th>\n",
              "      <th>Interest_Rate</th>\n",
              "      <th>Num_of_Loan</th>\n",
              "      <th>Delay_from_due_date</th>\n",
              "      <th>Num_of_Delayed_Payment</th>\n",
              "      <th>...</th>\n",
              "      <th>Monthly_Balance</th>\n",
              "      <th>AutoLoan</th>\n",
              "      <th>Credit-BuilderLoan</th>\n",
              "      <th>DebtConsolidationLoan</th>\n",
              "      <th>HomeEquityLoan</th>\n",
              "      <th>MortgageLoan</th>\n",
              "      <th>NotSpecified</th>\n",
              "      <th>PaydayLoan</th>\n",
              "      <th>PersonalLoan</th>\n",
              "      <th>StudentLoan</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23</td>\n",
              "      <td>Scientist</td>\n",
              "      <td>19114.12</td>\n",
              "      <td>1824.843333</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>186.266702</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24</td>\n",
              "      <td>Scientist</td>\n",
              "      <td>19114.12</td>\n",
              "      <td>1824.843333</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>9</td>\n",
              "      <td>...</td>\n",
              "      <td>361.444004</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24</td>\n",
              "      <td>Scientist</td>\n",
              "      <td>19114.12</td>\n",
              "      <td>4182.004291</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>343.826873</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>28</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>34847.84</td>\n",
              "      <td>3037.986667</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>303.355083</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>35</td>\n",
              "      <td>Engineer</td>\n",
              "      <td>143162.64</td>\n",
              "      <td>4182.004291</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1942</td>\n",
              "      <td>...</td>\n",
              "      <td>854.226027</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99aca5b0-47a5-4fb0-ae91-14dea9adcb7c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-99aca5b0-47a5-4fb0-ae91-14dea9adcb7c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-99aca5b0-47a5-4fb0-ae91-14dea9adcb7c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dbdb17d1-72d6-49cb-b81a-29cbd6225975\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dbdb17d1-72d6-49cb-b81a-29cbd6225975')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dbdb17d1-72d6-49cb-b81a-29cbd6225975 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fi5vQ0xJkTuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b6d750-0a24-4470-c8ee-57df75c3355d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Standard    13421\n",
              "Good         8963\n",
              "Bad          6839\n",
              "Name: Credit_Mix, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df[\"Credit_Mix\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZENObtyefVk"
      },
      "source": [
        "# 1. Preparing the data\n",
        "## 1.1 Data cleaning\n",
        " Perform OHE over the \"Occupation\" feature\n",
        "\n",
        " Then, perform LE over Payment_of_Min_Amount and Payment_Behaviour\n",
        "\n",
        " _hint: As we will be testing only one model no need to define a pipeline_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JGVrLNJTefVk"
      },
      "outputs": [],
      "source": [
        "# Your code here:\n",
        "# One-Hot Encoding for \"Occupation\"\n",
        "occupation_dummies = pd.get_dummies(df['Occupation'], prefix='Occupation')\n",
        "df = pd.concat([df, occupation_dummies], axis=1)\n",
        "df.drop('Occupation', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "\n",
        "# Label Encoding for 'Payment_of_Min_Amount'\n",
        "df['Payment_of_Min_Amount'] = le.fit_transform(df['Payment_of_Min_Amount'])\n",
        "\n",
        "# Label Encoding for 'Payment_Behaviour'\n",
        "df['Payment_Behaviour'] = le.fit_transform(df['Payment_Behaviour'])\n",
        "\n"
      ],
      "metadata": {
        "id": "Vha7G48lli5t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfPnhUxAefVl"
      },
      "source": [
        "## 1.2 Dataset splitting\n",
        "\n",
        "Split the dataset in two, first X with your independent features and then y with the dependent feature **CreditMix**.\n",
        "\n",
        "Then perform :\n",
        "* OneHotEncoding over the **CreditMix** feature.\n",
        "* A MinMaxScaller over the independent features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J-mentarefVm"
      },
      "outputs": [],
      "source": [
        "# Your code here:\n",
        "X = df.drop('Credit_Mix', axis=1)\n",
        "y = df['Credit_Mix']\n",
        "\n",
        "\n",
        "\n",
        "#Define the scaler\n",
        "y_dummies = pd.get_dummies(y, prefix='Credit_Mix')\n",
        "# Define the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the scaler on independent features\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert the scaled data back to a dataframe\n",
        "X = pd.DataFrame(X_scaled, columns=X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufKdhucZkTuE"
      },
      "source": [
        "### 1.2.1 Train Test splitting\n",
        "Now split the data in X_train, X_test, y_train, y_test,\n",
        "\n",
        "You can use test_size = 0.2 and a random_state of 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yotEvoAxefVn"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_dummies, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fZR_YlXkTuF"
      },
      "source": [
        "### 1.2.2 final touches\n",
        "Convert your datasets to `Torch tensors` of type `torch.float`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2KS_U8stefVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989e116d-c6ab-4873-b72a-fbc686d8be2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([23378, 42]) torch.Size([23378, 3])\n"
          ]
        }
      ],
      "source": [
        "#Your code here:\n",
        "# Convert training data\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float)\n",
        "\n",
        "# Convert testing data\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float)\n",
        "print(X_train_tensor.size(), y_train_tensor.size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5sRn97FkTuF"
      },
      "source": [
        "# 2 Model preparation:\n",
        "\n",
        "## 2.1 Define a Neural network model and instantiate it.\n",
        "You can set the number of neurons to 150."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8qqRoocVefVp"
      },
      "outputs": [],
      "source": [
        "# Define a neural network class here:\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # Define layers\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.layer2(x)  # No activation function here since we're assuming a classification task.\n",
        "        return x\n",
        "\n",
        "# Define input, hidden, and output sizes\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 150\n",
        "output_size = y_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cx-3yvp5efVp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8c1457-da99-4c92-869e-12b8abd15695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layer1): Linear(in_features=42, out_features=150, bias=True)\n",
            "  (layer2): Linear(in_features=150, out_features=3, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "6903\n"
          ]
        }
      ],
      "source": [
        "# Instantiate your model here\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "print(model)\n",
        "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4MBIrrfkTuF"
      },
      "source": [
        "## 2.2 finding the best model:\n",
        "Identify, amongst the following options the best parameters for your model:\n",
        "\n",
        "* `criterion` : [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), [BCEWithLogitsLoss](hhttps://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)\n",
        "* `iterations` : 150, 250, 500\n",
        "* `learning rate` : 0.00005, 0.001, 12.031\n",
        "\n",
        "\n",
        "_Hint: restart your runtime between each execution to ensure that previous neural networks dont interfere with your current one_\n",
        "\n",
        "_You can evaluate your model based on it's accuracy over the test set_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for best parameters and their accuracy\n",
        "best_params = {\n",
        "    \"criterion\": None,\n",
        "    \"iterations\": None,\n",
        "    \"learning_rate\": None,\n",
        "    \"accuracy\": 0\n",
        "}\n",
        "\n",
        "# Potential hyperparameters to search over\n",
        "criteria = [nn.CrossEntropyLoss, nn.BCEWithLogitsLoss]\n",
        "iterations_list = [150, 250, 500]\n",
        "learning_rates = [0.00005, 0.001, 12.031]\n",
        "\n",
        "for criterion in criteria:\n",
        "    for iters in iterations_list:\n",
        "        for lr in learning_rates:\n",
        "            # Instantiate the model\n",
        "            model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "            loss_function = criterion()\n",
        "\n",
        "            # Define Adam optimizer\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Training loop\n",
        "            for epoch in range(iters):\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(X_train_tensor)\n",
        "\n",
        "                if criterion == nn.CrossEntropyLoss:\n",
        "                    loss = loss_function(outputs, torch.max(y_train_tensor, 1)[1])\n",
        "                else:\n",
        "                    loss = loss_function(outputs, y_train_tensor)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # Evaluate accuracy\n",
        "            with torch.no_grad():\n",
        "                predicted = model(X_test_tensor)\n",
        "                if criterion == nn.CrossEntropyLoss:\n",
        "                    _, predicted_labels = torch.max(predicted, 1)\n",
        "                    _, true_labels = torch.max(y_test_tensor, 1)\n",
        "                    accuracy = (predicted_labels == true_labels).float().mean()\n",
        "                else:\n",
        "                    accuracy = ((predicted > 0.5) == y_test_tensor).float().mean()\n",
        "\n",
        "            # Print performance for the current combination\n",
        "            print(f\"Criterion: {criterion.__name__}, Iterations: {iters}, Learning Rate: {lr}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "            if accuracy > best_params[\"accuracy\"]:\n",
        "                best_params[\"criterion\"] = criterion\n",
        "                best_params[\"iterations\"] = iters\n",
        "                best_params[\"learning_rate\"] = lr\n",
        "                best_params[\"accuracy\"] = accuracy\n",
        "\n",
        "# Print the best parameters after all combinations\n",
        "print(\"\\nBest Parameters:\")\n",
        "print(best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkaCO76isGEY",
        "outputId": "b7c323fd-c878-4c91-dfe7-68d89c1cbb7c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Criterion: CrossEntropyLoss, Iterations: 150, Learning Rate: 5e-05, Accuracy: 0.4546\n",
            "Criterion: CrossEntropyLoss, Iterations: 150, Learning Rate: 0.001, Accuracy: 0.7701\n",
            "Criterion: CrossEntropyLoss, Iterations: 150, Learning Rate: 12.031, Accuracy: 0.4546\n",
            "Criterion: CrossEntropyLoss, Iterations: 250, Learning Rate: 5e-05, Accuracy: 0.4546\n",
            "Criterion: CrossEntropyLoss, Iterations: 250, Learning Rate: 0.001, Accuracy: 0.8029\n",
            "Criterion: CrossEntropyLoss, Iterations: 250, Learning Rate: 12.031, Accuracy: 0.4546\n",
            "Criterion: CrossEntropyLoss, Iterations: 500, Learning Rate: 5e-05, Accuracy: 0.4874\n",
            "Criterion: CrossEntropyLoss, Iterations: 500, Learning Rate: 0.001, Accuracy: 0.8392\n",
            "Criterion: CrossEntropyLoss, Iterations: 500, Learning Rate: 12.031, Accuracy: 0.4546\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 150, Learning Rate: 5e-05, Accuracy: 0.6667\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 150, Learning Rate: 0.001, Accuracy: 0.8143\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 150, Learning Rate: 12.031, Accuracy: 0.6667\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 250, Learning Rate: 5e-05, Accuracy: 0.6667\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 250, Learning Rate: 0.001, Accuracy: 0.8429\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 250, Learning Rate: 12.031, Accuracy: 0.6667\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 500, Learning Rate: 5e-05, Accuracy: 0.6667\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 500, Learning Rate: 0.001, Accuracy: 0.8717\n",
            "Criterion: BCEWithLogitsLoss, Iterations: 500, Learning Rate: 12.031, Accuracy: 0.6667\n",
            "\n",
            "Best Parameters:\n",
            "{'criterion': <class 'torch.nn.modules.loss.BCEWithLogitsLoss'>, 'iterations': 500, 'learning_rate': 0.001, 'accuracy': tensor(0.8717)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJQnLhzskTuG"
      },
      "source": [
        "## 2.3 Model Accuracy\n",
        "Identify the models accuracy over the train and test parts of the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Train Accuracy\n",
        "with torch.no_grad():\n",
        "    predicted_train = model(X_train_tensor)\n",
        "    if best_params[\"criterion\"] == nn.CrossEntropyLoss:\n",
        "        _, predicted_labels_train = torch.max(predicted_train, 1)\n",
        "        _, true_labels_train = torch.max(y_train_tensor, 1)\n",
        "        train_accuracy = (predicted_labels_train == true_labels_train).float().mean()\n",
        "    else:\n",
        "        train_accuracy = ((predicted_train > 0.5) == y_train_tensor).float().mean()\n",
        "\n",
        "# Evaluate Test Accuracy (You already have this as `best_params[\"accuracy\"]`)\n",
        "test_accuracy = best_params[\"accuracy\"]\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW09mlAOkME_",
        "outputId": "fb086ba2-798a-4707-adbf-b20adb61059a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.6667\n",
            "Test Accuracy: 0.8717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whoHkS_wkTuG"
      },
      "source": [
        "# 3. Predictions over the suspects dataset\n",
        "## 3.1 Retrain a new model over the full training dataset\n",
        "#### Please use the following parameters for this section:\n",
        "* ``neurons`` = 150\n",
        "* ``learning`` rate = 0.00005\n",
        "* ``criterion`` = CrossEntropyLoss\n",
        "* `iterations` = 500\n",
        "\n",
        "_hint you may have to redo some preprocessing as you did in part one_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new model here:\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 150)\n",
        "        self.layer2 = nn.Linear(150, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "model = NeuralNetwork()\n"
      ],
      "metadata": {
        "id": "PWVuKOL-5tgg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your MSE loss here:\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define your Adam optimizer for finding the weights of the network here:\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)"
      ],
      "metadata": {
        "id": "9xEvcog35tQB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform your training here\n",
        "for epoch in range(500):\n",
        "\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/500], Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxh1F6mV6BfZ",
        "outputId": "b75463e8-da3c-42f3-d760-d3cf1b878427"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 1.0874295234680176\n",
            "Epoch [2/500], Loss: 1.0871027708053589\n",
            "Epoch [3/500], Loss: 1.0867770910263062\n",
            "Epoch [4/500], Loss: 1.0864522457122803\n",
            "Epoch [5/500], Loss: 1.0861284732818604\n",
            "Epoch [6/500], Loss: 1.0858055353164673\n",
            "Epoch [7/500], Loss: 1.0854836702346802\n",
            "Epoch [8/500], Loss: 1.085162878036499\n",
            "Epoch [9/500], Loss: 1.0848430395126343\n",
            "Epoch [10/500], Loss: 1.0845242738723755\n",
            "Epoch [11/500], Loss: 1.0842065811157227\n",
            "Epoch [12/500], Loss: 1.0838898420333862\n",
            "Epoch [13/500], Loss: 1.0835742950439453\n",
            "Epoch [14/500], Loss: 1.0832598209381104\n",
            "Epoch [15/500], Loss: 1.0829461812973022\n",
            "Epoch [16/500], Loss: 1.0826337337493896\n",
            "Epoch [17/500], Loss: 1.082322359085083\n",
            "Epoch [18/500], Loss: 1.0820120573043823\n",
            "Epoch [19/500], Loss: 1.0817028284072876\n",
            "Epoch [20/500], Loss: 1.0813946723937988\n",
            "Epoch [21/500], Loss: 1.0810874700546265\n",
            "Epoch [22/500], Loss: 1.0807814598083496\n",
            "Epoch [23/500], Loss: 1.0804765224456787\n",
            "Epoch [24/500], Loss: 1.0801725387573242\n",
            "Epoch [25/500], Loss: 1.0798696279525757\n",
            "Epoch [26/500], Loss: 1.079567790031433\n",
            "Epoch [27/500], Loss: 1.0792670249938965\n",
            "Epoch [28/500], Loss: 1.0789672136306763\n",
            "Epoch [29/500], Loss: 1.0786685943603516\n",
            "Epoch [30/500], Loss: 1.0783709287643433\n",
            "Epoch [31/500], Loss: 1.0780742168426514\n",
            "Epoch [32/500], Loss: 1.0777783393859863\n",
            "Epoch [33/500], Loss: 1.0774836540222168\n",
            "Epoch [34/500], Loss: 1.0771899223327637\n",
            "Epoch [35/500], Loss: 1.0768970251083374\n",
            "Epoch [36/500], Loss: 1.0766050815582275\n",
            "Epoch [37/500], Loss: 1.0763142108917236\n",
            "Epoch [38/500], Loss: 1.0760242938995361\n",
            "Epoch [39/500], Loss: 1.075735330581665\n",
            "Epoch [40/500], Loss: 1.0754473209381104\n",
            "Epoch [41/500], Loss: 1.075160026550293\n",
            "Epoch [42/500], Loss: 1.074873685836792\n",
            "Epoch [43/500], Loss: 1.0745882987976074\n",
            "Epoch [44/500], Loss: 1.0743037462234497\n",
            "Epoch [45/500], Loss: 1.0740201473236084\n",
            "Epoch [46/500], Loss: 1.0737372636795044\n",
            "Epoch [47/500], Loss: 1.0734553337097168\n",
            "Epoch [48/500], Loss: 1.0731741189956665\n",
            "Epoch [49/500], Loss: 1.0728936195373535\n",
            "Epoch [50/500], Loss: 1.0726139545440674\n",
            "Epoch [51/500], Loss: 1.0723352432250977\n",
            "Epoch [52/500], Loss: 1.0720572471618652\n",
            "Epoch [53/500], Loss: 1.0717800855636597\n",
            "Epoch [54/500], Loss: 1.0715036392211914\n",
            "Epoch [55/500], Loss: 1.07122802734375\n",
            "Epoch [56/500], Loss: 1.070953130722046\n",
            "Epoch [57/500], Loss: 1.070678949356079\n",
            "Epoch [58/500], Loss: 1.0704054832458496\n",
            "Epoch [59/500], Loss: 1.070132851600647\n",
            "Epoch [60/500], Loss: 1.069860816001892\n",
            "Epoch [61/500], Loss: 1.0695894956588745\n",
            "Epoch [62/500], Loss: 1.0693190097808838\n",
            "Epoch [63/500], Loss: 1.0690490007400513\n",
            "Epoch [64/500], Loss: 1.0687799453735352\n",
            "Epoch [65/500], Loss: 1.0685112476348877\n",
            "Epoch [66/500], Loss: 1.068243384361267\n",
            "Epoch [67/500], Loss: 1.0679763555526733\n",
            "Epoch [68/500], Loss: 1.0677098035812378\n",
            "Epoch [69/500], Loss: 1.0674437284469604\n",
            "Epoch [70/500], Loss: 1.06717848777771\n",
            "Epoch [71/500], Loss: 1.0669138431549072\n",
            "Epoch [72/500], Loss: 1.0666497945785522\n",
            "Epoch [73/500], Loss: 1.0663859844207764\n",
            "Epoch [74/500], Loss: 1.0661232471466064\n",
            "Epoch [75/500], Loss: 1.0658607482910156\n",
            "Epoch [76/500], Loss: 1.065598726272583\n",
            "Epoch [77/500], Loss: 1.0653374195098877\n",
            "Epoch [78/500], Loss: 1.0650765895843506\n",
            "Epoch [79/500], Loss: 1.0648161172866821\n",
            "Epoch [80/500], Loss: 1.064556360244751\n",
            "Epoch [81/500], Loss: 1.064296841621399\n",
            "Epoch [82/500], Loss: 1.0640380382537842\n",
            "Epoch [83/500], Loss: 1.0637797117233276\n",
            "Epoch [84/500], Loss: 1.0635219812393188\n",
            "Epoch [85/500], Loss: 1.0632644891738892\n",
            "Epoch [86/500], Loss: 1.0630074739456177\n",
            "Epoch [87/500], Loss: 1.0627509355545044\n",
            "Epoch [88/500], Loss: 1.0624948740005493\n",
            "Epoch [89/500], Loss: 1.0622392892837524\n",
            "Epoch [90/500], Loss: 1.0619840621948242\n",
            "Epoch [91/500], Loss: 1.0617293119430542\n",
            "Epoch [92/500], Loss: 1.0614750385284424\n",
            "Epoch [93/500], Loss: 1.0612208843231201\n",
            "Epoch [94/500], Loss: 1.0609673261642456\n",
            "Epoch [95/500], Loss: 1.0607140064239502\n",
            "Epoch [96/500], Loss: 1.0604612827301025\n",
            "Epoch [97/500], Loss: 1.060208797454834\n",
            "Epoch [98/500], Loss: 1.059956669807434\n",
            "Epoch [99/500], Loss: 1.0597048997879028\n",
            "Epoch [100/500], Loss: 1.0594534873962402\n",
            "Epoch [101/500], Loss: 1.0592024326324463\n",
            "Epoch [102/500], Loss: 1.0589516162872314\n",
            "Epoch [103/500], Loss: 1.0587011575698853\n",
            "Epoch [104/500], Loss: 1.0584509372711182\n",
            "Epoch [105/500], Loss: 1.0582009553909302\n",
            "Epoch [106/500], Loss: 1.0579514503479004\n",
            "Epoch [107/500], Loss: 1.0577021837234497\n",
            "Epoch [108/500], Loss: 1.0574531555175781\n",
            "Epoch [109/500], Loss: 1.0572041273117065\n",
            "Epoch [110/500], Loss: 1.0569554567337036\n",
            "Epoch [111/500], Loss: 1.0567070245742798\n",
            "Epoch [112/500], Loss: 1.056458830833435\n",
            "Epoch [113/500], Loss: 1.0562108755111694\n",
            "Epoch [114/500], Loss: 1.0559632778167725\n",
            "Epoch [115/500], Loss: 1.055715560913086\n",
            "Epoch [116/500], Loss: 1.0554683208465576\n",
            "Epoch [117/500], Loss: 1.0552210807800293\n",
            "Epoch [118/500], Loss: 1.0549741983413696\n",
            "Epoch [119/500], Loss: 1.05472731590271\n",
            "Epoch [120/500], Loss: 1.0544806718826294\n",
            "Epoch [121/500], Loss: 1.054234266281128\n",
            "Epoch [122/500], Loss: 1.0539878606796265\n",
            "Epoch [123/500], Loss: 1.0537418127059937\n",
            "Epoch [124/500], Loss: 1.0534956455230713\n",
            "Epoch [125/500], Loss: 1.053249716758728\n",
            "Epoch [126/500], Loss: 1.0530040264129639\n",
            "Epoch [127/500], Loss: 1.0527585744857788\n",
            "Epoch [128/500], Loss: 1.052512764930725\n",
            "Epoch [129/500], Loss: 1.05226731300354\n",
            "Epoch [130/500], Loss: 1.052022099494934\n",
            "Epoch [131/500], Loss: 1.0517767667770386\n",
            "Epoch [132/500], Loss: 1.0515316724777222\n",
            "Epoch [133/500], Loss: 1.0512865781784058\n",
            "Epoch [134/500], Loss: 1.051041603088379\n",
            "Epoch [135/500], Loss: 1.0507965087890625\n",
            "Epoch [136/500], Loss: 1.0505516529083252\n",
            "Epoch [137/500], Loss: 1.050306797027588\n",
            "Epoch [138/500], Loss: 1.050061821937561\n",
            "Epoch [139/500], Loss: 1.0498170852661133\n",
            "Epoch [140/500], Loss: 1.049572229385376\n",
            "Epoch [141/500], Loss: 1.0493274927139282\n",
            "Epoch [142/500], Loss: 1.049082636833191\n",
            "Epoch [143/500], Loss: 1.0488379001617432\n",
            "Epoch [144/500], Loss: 1.048593282699585\n",
            "Epoch [145/500], Loss: 1.0483485460281372\n",
            "Epoch [146/500], Loss: 1.0481036901474\n",
            "Epoch [147/500], Loss: 1.0478589534759521\n",
            "Epoch [148/500], Loss: 1.0476140975952148\n",
            "Epoch [149/500], Loss: 1.0473692417144775\n",
            "Epoch [150/500], Loss: 1.0471242666244507\n",
            "Epoch [151/500], Loss: 1.0468794107437134\n",
            "Epoch [152/500], Loss: 1.046634554862976\n",
            "Epoch [153/500], Loss: 1.0463898181915283\n",
            "Epoch [154/500], Loss: 1.046144723892212\n",
            "Epoch [155/500], Loss: 1.0458993911743164\n",
            "Epoch [156/500], Loss: 1.045654296875\n",
            "Epoch [157/500], Loss: 1.045409083366394\n",
            "Epoch [158/500], Loss: 1.0451635122299194\n",
            "Epoch [159/500], Loss: 1.0449180603027344\n",
            "Epoch [160/500], Loss: 1.0446726083755493\n",
            "Epoch [161/500], Loss: 1.0444267988204956\n",
            "Epoch [162/500], Loss: 1.0441811084747314\n",
            "Epoch [163/500], Loss: 1.0439350605010986\n",
            "Epoch [164/500], Loss: 1.0436891317367554\n",
            "Epoch [165/500], Loss: 1.043442964553833\n",
            "Epoch [166/500], Loss: 1.043196678161621\n",
            "Epoch [167/500], Loss: 1.0429502725601196\n",
            "Epoch [168/500], Loss: 1.0427038669586182\n",
            "Epoch [169/500], Loss: 1.0424573421478271\n",
            "Epoch [170/500], Loss: 1.0422104597091675\n",
            "Epoch [171/500], Loss: 1.041963815689087\n",
            "Epoch [172/500], Loss: 1.0417166948318481\n",
            "Epoch [173/500], Loss: 1.0414695739746094\n",
            "Epoch [174/500], Loss: 1.041222095489502\n",
            "Epoch [175/500], Loss: 1.040974497795105\n",
            "Epoch [176/500], Loss: 1.040726661682129\n",
            "Epoch [177/500], Loss: 1.0404784679412842\n",
            "Epoch [178/500], Loss: 1.04023015499115\n",
            "Epoch [179/500], Loss: 1.039981722831726\n",
            "Epoch [180/500], Loss: 1.039732813835144\n",
            "Epoch [181/500], Loss: 1.0394837856292725\n",
            "Epoch [182/500], Loss: 1.0392343997955322\n",
            "Epoch [183/500], Loss: 1.038985013961792\n",
            "Epoch [184/500], Loss: 1.0387353897094727\n",
            "Epoch [185/500], Loss: 1.0384854078292847\n",
            "Epoch [186/500], Loss: 1.0382351875305176\n",
            "Epoch [187/500], Loss: 1.0379847288131714\n",
            "Epoch [188/500], Loss: 1.0377342700958252\n",
            "Epoch [189/500], Loss: 1.0374833345413208\n",
            "Epoch [190/500], Loss: 1.0372321605682373\n",
            "Epoch [191/500], Loss: 1.0369806289672852\n",
            "Epoch [192/500], Loss: 1.036728858947754\n",
            "Epoch [193/500], Loss: 1.0364768505096436\n",
            "Epoch [194/500], Loss: 1.036224603652954\n",
            "Epoch [195/500], Loss: 1.0359718799591064\n",
            "Epoch [196/500], Loss: 1.0357190370559692\n",
            "Epoch [197/500], Loss: 1.035465955734253\n",
            "Epoch [198/500], Loss: 1.035212516784668\n",
            "Epoch [199/500], Loss: 1.0349587202072144\n",
            "Epoch [200/500], Loss: 1.0347046852111816\n",
            "Epoch [201/500], Loss: 1.0344502925872803\n",
            "Epoch [202/500], Loss: 1.0341957807540894\n",
            "Epoch [203/500], Loss: 1.0339406728744507\n",
            "Epoch [204/500], Loss: 1.0336852073669434\n",
            "Epoch [205/500], Loss: 1.0334293842315674\n",
            "Epoch [206/500], Loss: 1.0331732034683228\n",
            "Epoch [207/500], Loss: 1.0329166650772095\n",
            "Epoch [208/500], Loss: 1.032659888267517\n",
            "Epoch [209/500], Loss: 1.0324023962020874\n",
            "Epoch [210/500], Loss: 1.0321444272994995\n",
            "Epoch [211/500], Loss: 1.031886339187622\n",
            "Epoch [212/500], Loss: 1.0316276550292969\n",
            "Epoch [213/500], Loss: 1.0313687324523926\n",
            "Epoch [214/500], Loss: 1.03110933303833\n",
            "Epoch [215/500], Loss: 1.0308492183685303\n",
            "Epoch [216/500], Loss: 1.0305888652801514\n",
            "Epoch [217/500], Loss: 1.0303279161453247\n",
            "Epoch [218/500], Loss: 1.0300666093826294\n",
            "Epoch [219/500], Loss: 1.0298048257827759\n",
            "Epoch [220/500], Loss: 1.0295424461364746\n",
            "Epoch [221/500], Loss: 1.0292797088623047\n",
            "Epoch [222/500], Loss: 1.029016375541687\n",
            "Epoch [223/500], Loss: 1.0287526845932007\n",
            "Epoch [224/500], Loss: 1.0284885168075562\n",
            "Epoch [225/500], Loss: 1.028223991394043\n",
            "Epoch [226/500], Loss: 1.027958869934082\n",
            "Epoch [227/500], Loss: 1.0276931524276733\n",
            "Epoch [228/500], Loss: 1.0274269580841064\n",
            "Epoch [229/500], Loss: 1.027160406112671\n",
            "Epoch [230/500], Loss: 1.026893138885498\n",
            "Epoch [231/500], Loss: 1.0266252756118774\n",
            "Epoch [232/500], Loss: 1.0263569355010986\n",
            "Epoch [233/500], Loss: 1.026087999343872\n",
            "Epoch [234/500], Loss: 1.0258184671401978\n",
            "Epoch [235/500], Loss: 1.0255483388900757\n",
            "Epoch [236/500], Loss: 1.0252779722213745\n",
            "Epoch [237/500], Loss: 1.0250067710876465\n",
            "Epoch [238/500], Loss: 1.0247352123260498\n",
            "Epoch [239/500], Loss: 1.0244630575180054\n",
            "Epoch [240/500], Loss: 1.0241905450820923\n",
            "Epoch [241/500], Loss: 1.0239170789718628\n",
            "Epoch [242/500], Loss: 1.023643136024475\n",
            "Epoch [243/500], Loss: 1.0233688354492188\n",
            "Epoch [244/500], Loss: 1.023093819618225\n",
            "Epoch [245/500], Loss: 1.0228182077407837\n",
            "Epoch [246/500], Loss: 1.022541880607605\n",
            "Epoch [247/500], Loss: 1.022265076637268\n",
            "Epoch [248/500], Loss: 1.0219874382019043\n",
            "Epoch [249/500], Loss: 1.0217093229293823\n",
            "Epoch [250/500], Loss: 1.021430492401123\n",
            "Epoch [251/500], Loss: 1.021151065826416\n",
            "Epoch [252/500], Loss: 1.0208712816238403\n",
            "Epoch [253/500], Loss: 1.0205906629562378\n",
            "Epoch [254/500], Loss: 1.020309567451477\n",
            "Epoch [255/500], Loss: 1.0200275182724\n",
            "Epoch [256/500], Loss: 1.0197449922561646\n",
            "Epoch [257/500], Loss: 1.0194618701934814\n",
            "Epoch [258/500], Loss: 1.019178032875061\n",
            "Epoch [259/500], Loss: 1.0188935995101929\n",
            "Epoch [260/500], Loss: 1.018608570098877\n",
            "Epoch [261/500], Loss: 1.0183228254318237\n",
            "Epoch [262/500], Loss: 1.0180364847183228\n",
            "Epoch [263/500], Loss: 1.0177494287490845\n",
            "Epoch [264/500], Loss: 1.0174617767333984\n",
            "Epoch [265/500], Loss: 1.0171732902526855\n",
            "Epoch [266/500], Loss: 1.0168843269348145\n",
            "Epoch [267/500], Loss: 1.016594648361206\n",
            "Epoch [268/500], Loss: 1.0163041353225708\n",
            "Epoch [269/500], Loss: 1.016013264656067\n",
            "Epoch [270/500], Loss: 1.015721321105957\n",
            "Epoch [271/500], Loss: 1.0154287815093994\n",
            "Epoch [272/500], Loss: 1.0151357650756836\n",
            "Epoch [273/500], Loss: 1.0148417949676514\n",
            "Epoch [274/500], Loss: 1.0145474672317505\n",
            "Epoch [275/500], Loss: 1.0142523050308228\n",
            "Epoch [276/500], Loss: 1.0139565467834473\n",
            "Epoch [277/500], Loss: 1.013659954071045\n",
            "Epoch [278/500], Loss: 1.0133631229400635\n",
            "Epoch [279/500], Loss: 1.0130653381347656\n",
            "Epoch [280/500], Loss: 1.01276695728302\n",
            "Epoch [281/500], Loss: 1.012467861175537\n",
            "Epoch [282/500], Loss: 1.012168049812317\n",
            "Epoch [283/500], Loss: 1.0118675231933594\n",
            "Epoch [284/500], Loss: 1.011566400527954\n",
            "Epoch [285/500], Loss: 1.0112648010253906\n",
            "Epoch [286/500], Loss: 1.0109623670578003\n",
            "Epoch [287/500], Loss: 1.0106592178344727\n",
            "Epoch [288/500], Loss: 1.0103553533554077\n",
            "Epoch [289/500], Loss: 1.0100507736206055\n",
            "Epoch [290/500], Loss: 1.009745478630066\n",
            "Epoch [291/500], Loss: 1.0094395875930786\n",
            "Epoch [292/500], Loss: 1.009132742881775\n",
            "Epoch [293/500], Loss: 1.0088251829147339\n",
            "Epoch [294/500], Loss: 1.0085169076919556\n",
            "Epoch [295/500], Loss: 1.0082080364227295\n",
            "Epoch [296/500], Loss: 1.007898211479187\n",
            "Epoch [297/500], Loss: 1.0075877904891968\n",
            "Epoch [298/500], Loss: 1.0072765350341797\n",
            "Epoch [299/500], Loss: 1.0069645643234253\n",
            "Epoch [300/500], Loss: 1.0066519975662231\n",
            "Epoch [301/500], Loss: 1.0063385963439941\n",
            "Epoch [302/500], Loss: 1.0060243606567383\n",
            "Epoch [303/500], Loss: 1.0057095289230347\n",
            "Epoch [304/500], Loss: 1.0053938627243042\n",
            "Epoch [305/500], Loss: 1.0050773620605469\n",
            "Epoch [306/500], Loss: 1.0047603845596313\n",
            "Epoch [307/500], Loss: 1.0044426918029785\n",
            "Epoch [308/500], Loss: 1.0041241645812988\n",
            "Epoch [309/500], Loss: 1.0038050413131714\n",
            "Epoch [310/500], Loss: 1.0034849643707275\n",
            "Epoch [311/500], Loss: 1.0031644105911255\n",
            "Epoch [312/500], Loss: 1.0028430223464966\n",
            "Epoch [313/500], Loss: 1.00252103805542\n",
            "Epoch [314/500], Loss: 1.0021981000900269\n",
            "Epoch [315/500], Loss: 1.001874566078186\n",
            "Epoch [316/500], Loss: 1.0015500783920288\n",
            "Epoch [317/500], Loss: 1.0012251138687134\n",
            "Epoch [318/500], Loss: 1.0008991956710815\n",
            "Epoch [319/500], Loss: 1.000572681427002\n",
            "Epoch [320/500], Loss: 1.0002453327178955\n",
            "Epoch [321/500], Loss: 0.9999174475669861\n",
            "Epoch [322/500], Loss: 0.9995887279510498\n",
            "Epoch [323/500], Loss: 0.9992592930793762\n",
            "Epoch [324/500], Loss: 0.9989292025566101\n",
            "Epoch [325/500], Loss: 0.9985982775688171\n",
            "Epoch [326/500], Loss: 0.998266875743866\n",
            "Epoch [327/500], Loss: 0.9979345798492432\n",
            "Epoch [328/500], Loss: 0.9976016879081726\n",
            "Epoch [329/500], Loss: 0.9972679018974304\n",
            "Epoch [330/500], Loss: 0.9969334006309509\n",
            "Epoch [331/500], Loss: 0.9965981841087341\n",
            "Epoch [332/500], Loss: 0.99626225233078\n",
            "Epoch [333/500], Loss: 0.9959256649017334\n",
            "Epoch [334/500], Loss: 0.9955885410308838\n",
            "Epoch [335/500], Loss: 0.9952505230903625\n",
            "Epoch [336/500], Loss: 0.9949118494987488\n",
            "Epoch [337/500], Loss: 0.9945725798606873\n",
            "Epoch [338/500], Loss: 0.994232714176178\n",
            "Epoch [339/500], Loss: 0.9938920736312866\n",
            "Epoch [340/500], Loss: 0.9935507774353027\n",
            "Epoch [341/500], Loss: 0.9932088255882263\n",
            "Epoch [342/500], Loss: 0.9928662776947021\n",
            "Epoch [343/500], Loss: 0.992523193359375\n",
            "Epoch [344/500], Loss: 0.9921793937683105\n",
            "Epoch [345/500], Loss: 0.9918349981307983\n",
            "Epoch [346/500], Loss: 0.9914898872375488\n",
            "Epoch [347/500], Loss: 0.9911441206932068\n",
            "Epoch [348/500], Loss: 0.9907976388931274\n",
            "Epoch [349/500], Loss: 0.9904504418373108\n",
            "Epoch [350/500], Loss: 0.9901026487350464\n",
            "Epoch [351/500], Loss: 0.9897542595863342\n",
            "Epoch [352/500], Loss: 0.9894052147865295\n",
            "Epoch [353/500], Loss: 0.9890554547309875\n",
            "Epoch [354/500], Loss: 0.9887051582336426\n",
            "Epoch [355/500], Loss: 0.9883542060852051\n",
            "Epoch [356/500], Loss: 0.988002598285675\n",
            "Epoch [357/500], Loss: 0.987650454044342\n",
            "Epoch [358/500], Loss: 0.987297773361206\n",
            "Epoch [359/500], Loss: 0.9869440197944641\n",
            "Epoch [360/500], Loss: 0.9865899682044983\n",
            "Epoch [361/500], Loss: 0.9862351417541504\n",
            "Epoch [362/500], Loss: 0.9858797192573547\n",
            "Epoch [363/500], Loss: 0.9855235815048218\n",
            "Epoch [364/500], Loss: 0.9851668477058411\n",
            "Epoch [365/500], Loss: 0.9848095178604126\n",
            "Epoch [366/500], Loss: 0.9844514727592468\n",
            "Epoch [367/500], Loss: 0.9840927720069885\n",
            "Epoch [368/500], Loss: 0.9837332963943481\n",
            "Epoch [369/500], Loss: 0.9833732843399048\n",
            "Epoch [370/500], Loss: 0.9830126166343689\n",
            "Epoch [371/500], Loss: 0.9826512932777405\n",
            "Epoch [372/500], Loss: 0.9822893738746643\n",
            "Epoch [373/500], Loss: 0.9819267988204956\n",
            "Epoch [374/500], Loss: 0.9815636873245239\n",
            "Epoch [375/500], Loss: 0.9811996817588806\n",
            "Epoch [376/500], Loss: 0.9808351993560791\n",
            "Epoch [377/500], Loss: 0.9804701805114746\n",
            "Epoch [378/500], Loss: 0.9801045656204224\n",
            "Epoch [379/500], Loss: 0.9797382950782776\n",
            "Epoch [380/500], Loss: 0.9793713092803955\n",
            "Epoch [381/500], Loss: 0.9790038466453552\n",
            "Epoch [382/500], Loss: 0.978635847568512\n",
            "Epoch [383/500], Loss: 0.9782673120498657\n",
            "Epoch [384/500], Loss: 0.9778982400894165\n",
            "Epoch [385/500], Loss: 0.97752845287323\n",
            "Epoch [386/500], Loss: 0.97715824842453\n",
            "Epoch [387/500], Loss: 0.9767875075340271\n",
            "Epoch [388/500], Loss: 0.9764161109924316\n",
            "Epoch [389/500], Loss: 0.9760439991950989\n",
            "Epoch [390/500], Loss: 0.9756715893745422\n",
            "Epoch [391/500], Loss: 0.9752985239028931\n",
            "Epoch [392/500], Loss: 0.9749248623847961\n",
            "Epoch [393/500], Loss: 0.9745506644248962\n",
            "Epoch [394/500], Loss: 0.9741759300231934\n",
            "Epoch [395/500], Loss: 0.9738004803657532\n",
            "Epoch [396/500], Loss: 0.9734246134757996\n",
            "Epoch [397/500], Loss: 0.9730479717254639\n",
            "Epoch [398/500], Loss: 0.9726709723472595\n",
            "Epoch [399/500], Loss: 0.9722933173179626\n",
            "Epoch [400/500], Loss: 0.9719149470329285\n",
            "Epoch [401/500], Loss: 0.9715361595153809\n",
            "Epoch [402/500], Loss: 0.9711567163467407\n",
            "Epoch [403/500], Loss: 0.9707767367362976\n",
            "Epoch [404/500], Loss: 0.9703961610794067\n",
            "Epoch [405/500], Loss: 0.9700150489807129\n",
            "Epoch [406/500], Loss: 0.9696334004402161\n",
            "Epoch [407/500], Loss: 0.9692513346672058\n",
            "Epoch [408/500], Loss: 0.9688685536384583\n",
            "Epoch [409/500], Loss: 0.9684853553771973\n",
            "Epoch [410/500], Loss: 0.9681015014648438\n",
            "Epoch [411/500], Loss: 0.9677170515060425\n",
            "Epoch [412/500], Loss: 0.9673320651054382\n",
            "Epoch [413/500], Loss: 0.9669464230537415\n",
            "Epoch [414/500], Loss: 0.9665600061416626\n",
            "Epoch [415/500], Loss: 0.9661732316017151\n",
            "Epoch [416/500], Loss: 0.9657857418060303\n",
            "Epoch [417/500], Loss: 0.9653976559638977\n",
            "Epoch [418/500], Loss: 0.9650091528892517\n",
            "Epoch [419/500], Loss: 0.9646198749542236\n",
            "Epoch [420/500], Loss: 0.9642301201820374\n",
            "Epoch [421/500], Loss: 0.9638397097587585\n",
            "Epoch [422/500], Loss: 0.9634489417076111\n",
            "Epoch [423/500], Loss: 0.9630576372146606\n",
            "Epoch [424/500], Loss: 0.9626655578613281\n",
            "Epoch [425/500], Loss: 0.962273120880127\n",
            "Epoch [426/500], Loss: 0.9618802070617676\n",
            "Epoch [427/500], Loss: 0.9614865779876709\n",
            "Epoch [428/500], Loss: 0.9610924124717712\n",
            "Epoch [429/500], Loss: 0.9606977105140686\n",
            "Epoch [430/500], Loss: 0.9603025317192078\n",
            "Epoch [431/500], Loss: 0.9599068760871887\n",
            "Epoch [432/500], Loss: 0.9595106244087219\n",
            "Epoch [433/500], Loss: 0.9591138958930969\n",
            "Epoch [434/500], Loss: 0.958716630935669\n",
            "Epoch [435/500], Loss: 0.9583187699317932\n",
            "Epoch [436/500], Loss: 0.9579203128814697\n",
            "Epoch [437/500], Loss: 0.9575215578079224\n",
            "Epoch [438/500], Loss: 0.9571223258972168\n",
            "Epoch [439/500], Loss: 0.956722617149353\n",
            "Epoch [440/500], Loss: 0.956322431564331\n",
            "Epoch [441/500], Loss: 0.9559218287467957\n",
            "Epoch [442/500], Loss: 0.9555206894874573\n",
            "Epoch [443/500], Loss: 0.9551189541816711\n",
            "Epoch [444/500], Loss: 0.954716682434082\n",
            "Epoch [445/500], Loss: 0.9543139934539795\n",
            "Epoch [446/500], Loss: 0.9539110660552979\n",
            "Epoch [447/500], Loss: 0.9535074830055237\n",
            "Epoch [448/500], Loss: 0.9531036019325256\n",
            "Epoch [449/500], Loss: 0.9526990056037903\n",
            "Epoch [450/500], Loss: 0.9522940516471863\n",
            "Epoch [451/500], Loss: 0.9518884420394897\n",
            "Epoch [452/500], Loss: 0.9514826536178589\n",
            "Epoch [453/500], Loss: 0.9510762095451355\n",
            "Epoch [454/500], Loss: 0.9506694078445435\n",
            "Epoch [455/500], Loss: 0.9502620697021484\n",
            "Epoch [456/500], Loss: 0.9498543739318848\n",
            "Epoch [457/500], Loss: 0.9494462609291077\n",
            "Epoch [458/500], Loss: 0.9490377306938171\n",
            "Epoch [459/500], Loss: 0.9486284852027893\n",
            "Epoch [460/500], Loss: 0.948218822479248\n",
            "Epoch [461/500], Loss: 0.9478088617324829\n",
            "Epoch [462/500], Loss: 0.9473983645439148\n",
            "Epoch [463/500], Loss: 0.9469873905181885\n",
            "Epoch [464/500], Loss: 0.946575939655304\n",
            "Epoch [465/500], Loss: 0.946164071559906\n",
            "Epoch [466/500], Loss: 0.9457516074180603\n",
            "Epoch [467/500], Loss: 0.9453389048576355\n",
            "Epoch [468/500], Loss: 0.9449255466461182\n",
            "Epoch [469/500], Loss: 0.9445117115974426\n",
            "Epoch [470/500], Loss: 0.9440975189208984\n",
            "Epoch [471/500], Loss: 0.9436827898025513\n",
            "Epoch [472/500], Loss: 0.9432674646377563\n",
            "Epoch [473/500], Loss: 0.9428517818450928\n",
            "Epoch [474/500], Loss: 0.9424356818199158\n",
            "Epoch [475/500], Loss: 0.942018985748291\n",
            "Epoch [476/500], Loss: 0.9416019320487976\n",
            "Epoch [477/500], Loss: 0.9411842823028564\n",
            "Epoch [478/500], Loss: 0.9407660961151123\n",
            "Epoch [479/500], Loss: 0.9403476715087891\n",
            "Epoch [480/500], Loss: 0.9399288296699524\n",
            "Epoch [481/500], Loss: 0.9395092725753784\n",
            "Epoch [482/500], Loss: 0.9390895366668701\n",
            "Epoch [483/500], Loss: 0.9386692047119141\n",
            "Epoch [484/500], Loss: 0.9382483959197998\n",
            "Epoch [485/500], Loss: 0.9378272294998169\n",
            "Epoch [486/500], Loss: 0.9374057054519653\n",
            "Epoch [487/500], Loss: 0.9369837641716003\n",
            "Epoch [488/500], Loss: 0.9365612864494324\n",
            "Epoch [489/500], Loss: 0.9361384510993958\n",
            "Epoch [490/500], Loss: 0.9357153177261353\n",
            "Epoch [491/500], Loss: 0.9352916479110718\n",
            "Epoch [492/500], Loss: 0.9348676800727844\n",
            "Epoch [493/500], Loss: 0.9344434142112732\n",
            "Epoch [494/500], Loss: 0.9340186715126038\n",
            "Epoch [495/500], Loss: 0.9335935115814209\n",
            "Epoch [496/500], Loss: 0.9331679940223694\n",
            "Epoch [497/500], Loss: 0.9327419400215149\n",
            "Epoch [498/500], Loss: 0.9323155879974365\n",
            "Epoch [499/500], Loss: 0.9318888187408447\n",
            "Epoch [500/500], Loss: 0.93146151304245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ5-dtCokTuH"
      },
      "source": [
        "## 3.2 Predict over the suspects dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "suspect_ids = suspects['userID']"
      ],
      "metadata": {
        "id": "cElCCcKw86TC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suspects_for_prediction = suspects.drop(columns=['userID'])\n",
        "suspects_tensor = torch.FloatTensor(suspects_for_prediction.values)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(suspects_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n"
      ],
      "metadata": {
        "id": "SJhr9xcX5uQd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions to a list or array\n",
        "predicted_list = predicted.numpy()\n",
        "\n",
        "# Create a DataFrame to map 'suspectID' to predictions\n",
        "results_df = pd.DataFrame({\n",
        "    'suspectID': suspect_ids,\n",
        "    'Prediction': predicted_list\n",
        "})\n",
        "\n",
        "# Identify which users have a good credit score\n",
        "good_credit_score_users = results_df[results_df['Prediction'] == 1]['suspectID'].tolist()\n",
        "good_credit_score_users"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPqNqSNM91aw",
        "outputId": "3fff3182-aec3-4db2-dae3-f2fc79941b38"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[317991,\n",
              " 241892,\n",
              " 303376,\n",
              " 761992,\n",
              " 373318,\n",
              " 938770,\n",
              " 676003,\n",
              " 255073,\n",
              " 468560,\n",
              " 263592,\n",
              " 660506,\n",
              " 244162,\n",
              " 774730,\n",
              " 911014,\n",
              " 601633,\n",
              " 645054,\n",
              " 255830,\n",
              " 353527,\n",
              " 531937,\n",
              " 413250,\n",
              " 601958,\n",
              " 596063,\n",
              " 764821,\n",
              " 880689,\n",
              " 340001,\n",
              " 512604,\n",
              " 165305,\n",
              " 334769,\n",
              " 564061,\n",
              " 306495,\n",
              " 647912,\n",
              " 946059,\n",
              " 917315,\n",
              " 504629,\n",
              " 520229,\n",
              " 861915,\n",
              " 308845,\n",
              " 654987,\n",
              " 398531,\n",
              " 327507,\n",
              " 888670,\n",
              " 449514,\n",
              " 819045,\n",
              " 899768,\n",
              " 251233,\n",
              " 344946,\n",
              " 932285,\n",
              " 169664,\n",
              " 265615,\n",
              " 867078,\n",
              " 618568,\n",
              " 512128,\n",
              " 222875,\n",
              " 160212,\n",
              " 466624,\n",
              " 903438,\n",
              " 957997,\n",
              " 362662,\n",
              " 892546,\n",
              " 777470,\n",
              " 397912,\n",
              " 123562,\n",
              " 939524,\n",
              " 387676,\n",
              " 860681,\n",
              " 789726,\n",
              " 167822,\n",
              " 571370,\n",
              " 863210,\n",
              " 717780,\n",
              " 863592,\n",
              " 898254,\n",
              " 424603,\n",
              " 632973,\n",
              " 599462,\n",
              " 601265,\n",
              " 402663,\n",
              " 226609,\n",
              " 790344,\n",
              " 862416,\n",
              " 134514,\n",
              " 177842,\n",
              " 551506,\n",
              " 365406,\n",
              " 825733,\n",
              " 220420,\n",
              " 927140,\n",
              " 596661,\n",
              " 688784,\n",
              " 829946,\n",
              " 242912,\n",
              " 443524,\n",
              " 464891,\n",
              " 380835,\n",
              " 823355,\n",
              " 456546,\n",
              " 790046,\n",
              " 932281,\n",
              " 538431,\n",
              " 297653,\n",
              " 206269,\n",
              " 96336,\n",
              " 273453,\n",
              " 112768,\n",
              " 446376,\n",
              " 205381,\n",
              " 614048,\n",
              " 931353,\n",
              " 773737,\n",
              " 793505,\n",
              " 419563,\n",
              " 299485,\n",
              " 752845,\n",
              " 278360,\n",
              " 494514,\n",
              " 571364,\n",
              " 303529,\n",
              " 796491,\n",
              " 205775,\n",
              " 133154,\n",
              " 182040,\n",
              " 425341,\n",
              " 778887,\n",
              " 583984,\n",
              " 539227,\n",
              " 676494,\n",
              " 430864,\n",
              " 899581,\n",
              " 240613,\n",
              " 859112,\n",
              " 541833,\n",
              " 126115,\n",
              " 839591,\n",
              " 453712,\n",
              " 544861,\n",
              " 544078,\n",
              " 183509,\n",
              " 259083,\n",
              " 512432,\n",
              " 874150,\n",
              " 484182,\n",
              " 745938,\n",
              " 291359,\n",
              " 499672,\n",
              " 757865,\n",
              " 744124,\n",
              " 641947,\n",
              " 631749,\n",
              " 306277,\n",
              " 235799,\n",
              " 710764,\n",
              " 781819,\n",
              " 666304,\n",
              " 936933,\n",
              " 553754,\n",
              " 549118,\n",
              " 700708,\n",
              " 817857,\n",
              " 874839,\n",
              " 693763,\n",
              " 367673,\n",
              " 619595,\n",
              " 103928,\n",
              " 245167,\n",
              " 893389,\n",
              " 397601,\n",
              " 442794,\n",
              " 949985,\n",
              " 422134,\n",
              " 852073,\n",
              " 822072,\n",
              " 628503,\n",
              " 953342,\n",
              " 397921,\n",
              " 927269,\n",
              " 228819,\n",
              " 913560,\n",
              " 648234,\n",
              " 294336,\n",
              " 943792,\n",
              " 267733,\n",
              " 638192,\n",
              " 353596,\n",
              " 350035,\n",
              " 570734,\n",
              " 743377,\n",
              " 605766,\n",
              " 957079,\n",
              " 150642,\n",
              " 356865,\n",
              " 621691,\n",
              " 888216,\n",
              " 210667,\n",
              " 794594,\n",
              " 524465,\n",
              " 740078,\n",
              " 754702,\n",
              " 429697,\n",
              " 741709,\n",
              " 510420,\n",
              " 204180,\n",
              " 564884,\n",
              " 412064,\n",
              " 490146,\n",
              " 881695,\n",
              " 933156,\n",
              " 107841,\n",
              " 416507,\n",
              " 441791,\n",
              " 565225,\n",
              " 303568,\n",
              " 304619,\n",
              " 866911,\n",
              " 168991,\n",
              " 649442,\n",
              " 754989,\n",
              " 937535,\n",
              " 376743,\n",
              " 767527,\n",
              " 225178,\n",
              " 519735,\n",
              " 953642,\n",
              " 125746,\n",
              " 623880,\n",
              " 720728,\n",
              " 836925,\n",
              " 449172,\n",
              " 198680,\n",
              " 428637,\n",
              " 702124,\n",
              " 706286,\n",
              " 934741,\n",
              " 792996,\n",
              " 391296,\n",
              " 318978,\n",
              " 149578,\n",
              " 156341,\n",
              " 140991,\n",
              " 641389,\n",
              " 421513,\n",
              " 460980,\n",
              " 184231,\n",
              " 355972,\n",
              " 806187,\n",
              " 628233,\n",
              " 522966,\n",
              " 925217,\n",
              " 535048,\n",
              " 168728,\n",
              " 241404,\n",
              " 703326,\n",
              " 325946,\n",
              " 154013,\n",
              " 448665,\n",
              " 814931,\n",
              " 537543,\n",
              " 803332,\n",
              " 581364,\n",
              " 455352,\n",
              " 128462,\n",
              " 151092,\n",
              " 875439,\n",
              " 418245,\n",
              " 843485,\n",
              " 99489,\n",
              " 231948,\n",
              " 645264,\n",
              " 649131,\n",
              " 382851,\n",
              " 108420,\n",
              " 339524,\n",
              " 509130,\n",
              " 719655,\n",
              " 316652,\n",
              " 653321,\n",
              " 569590,\n",
              " 517645,\n",
              " 481578,\n",
              " 438895,\n",
              " 942442,\n",
              " 304526,\n",
              " 393146,\n",
              " 434361,\n",
              " 548974,\n",
              " 726678,\n",
              " 804662,\n",
              " 456436,\n",
              " 661944,\n",
              " 108215,\n",
              " 378721,\n",
              " 573810,\n",
              " 769196,\n",
              " 804201,\n",
              " 395115,\n",
              " 287607,\n",
              " 866984,\n",
              " 791891,\n",
              " 631142,\n",
              " 780084,\n",
              " 708168,\n",
              " 131191,\n",
              " 474811,\n",
              " 490705,\n",
              " 898063,\n",
              " 409012,\n",
              " 610905,\n",
              " 621836,\n",
              " 387404,\n",
              " 454379,\n",
              " 503392,\n",
              " 859745,\n",
              " 931183,\n",
              " 780199,\n",
              " 651597,\n",
              " 411537,\n",
              " 795804,\n",
              " 431634,\n",
              " 883252,\n",
              " 857627,\n",
              " 795805,\n",
              " 372166,\n",
              " 581173,\n",
              " 98459,\n",
              " 280659,\n",
              " 536355,\n",
              " 431378,\n",
              " 935949,\n",
              " 201125,\n",
              " 304338,\n",
              " 704873,\n",
              " 131824,\n",
              " 204834,\n",
              " 172656,\n",
              " 527013,\n",
              " 424791,\n",
              " 465159,\n",
              " 132322,\n",
              " 861181,\n",
              " 455736,\n",
              " 422474,\n",
              " 411464,\n",
              " 415269,\n",
              " 119792,\n",
              " 788180,\n",
              " 713550,\n",
              " 347349,\n",
              " 427246,\n",
              " 589319,\n",
              " 509348,\n",
              " 383876,\n",
              " 439819,\n",
              " 887417,\n",
              " 258345,\n",
              " 792062,\n",
              " 351472,\n",
              " 416130,\n",
              " 694183,\n",
              " 231770,\n",
              " 679196,\n",
              " 425350,\n",
              " 544592,\n",
              " 550287,\n",
              " 729575,\n",
              " 813808,\n",
              " 597612,\n",
              " 826577,\n",
              " 951822,\n",
              " 352848,\n",
              " 219849,\n",
              " 183438,\n",
              " 339793,\n",
              " 131393,\n",
              " 410319,\n",
              " 793674,\n",
              " 386917,\n",
              " 200688,\n",
              " 382237,\n",
              " 224422,\n",
              " 772645,\n",
              " 169705,\n",
              " 638950,\n",
              " 698563,\n",
              " 949973,\n",
              " 643332,\n",
              " 486395,\n",
              " 915055,\n",
              " 505876,\n",
              " 673325,\n",
              " 223968,\n",
              " 96722,\n",
              " 902629,\n",
              " 531904,\n",
              " 128444,\n",
              " 233577,\n",
              " 123049,\n",
              " 437181,\n",
              " 628925,\n",
              " 376598,\n",
              " 919979,\n",
              " 897796,\n",
              " 907542,\n",
              " 571155,\n",
              " 257731,\n",
              " 213269,\n",
              " 269302,\n",
              " 623217,\n",
              " 789033,\n",
              " 601927,\n",
              " 674497,\n",
              " 474746,\n",
              " 739432,\n",
              " 793023,\n",
              " 724380,\n",
              " 682811,\n",
              " 520750,\n",
              " 637567,\n",
              " 806987,\n",
              " 128570,\n",
              " 945010,\n",
              " 748037,\n",
              " 867548,\n",
              " 453185,\n",
              " 586186,\n",
              " 886132,\n",
              " 362504,\n",
              " 115712,\n",
              " 410221,\n",
              " 750955,\n",
              " 253352,\n",
              " 947685,\n",
              " 802042,\n",
              " 952903,\n",
              " 948037,\n",
              " 569177,\n",
              " 404377,\n",
              " 947658,\n",
              " 96840,\n",
              " 131489,\n",
              " 917041,\n",
              " 739909,\n",
              " 284349,\n",
              " 188061,\n",
              " 784524,\n",
              " 354752,\n",
              " 325666,\n",
              " 223665,\n",
              " 846622,\n",
              " 285653,\n",
              " 543239,\n",
              " 623888,\n",
              " 926004,\n",
              " 553784,\n",
              " 485010,\n",
              " 648842,\n",
              " 556033,\n",
              " 931749,\n",
              " 334386,\n",
              " 843464,\n",
              " 333546,\n",
              " 242361,\n",
              " 333546,\n",
              " 802773,\n",
              " 444482,\n",
              " 139463,\n",
              " 833402,\n",
              " 102671,\n",
              " 711683,\n",
              " 843660,\n",
              " 501454,\n",
              " 466702,\n",
              " 208948,\n",
              " 587706,\n",
              " 918924,\n",
              " 245949,\n",
              " 803882,\n",
              " 623440,\n",
              " 405604,\n",
              " 138796,\n",
              " 739300,\n",
              " 232096,\n",
              " 765508,\n",
              " 872993,\n",
              " 190042,\n",
              " 119767,\n",
              " 655000,\n",
              " 332018,\n",
              " 515957,\n",
              " 809385,\n",
              " 403139,\n",
              " 261521,\n",
              " 245757,\n",
              " 267120,\n",
              " 428573,\n",
              " 284274,\n",
              " 949262,\n",
              " 520948,\n",
              " 208153,\n",
              " 136015,\n",
              " 773146,\n",
              " 820116,\n",
              " 410444,\n",
              " 128800,\n",
              " 327047,\n",
              " 413270,\n",
              " 790265,\n",
              " 152304,\n",
              " 572046,\n",
              " 651176,\n",
              " 418505,\n",
              " 476861,\n",
              " 906609,\n",
              " 874746,\n",
              " 354164,\n",
              " 837811,\n",
              " 818249,\n",
              " 371132,\n",
              " 702840,\n",
              " 654139,\n",
              " 123006,\n",
              " 344301,\n",
              " 299520,\n",
              " 165762,\n",
              " 742635,\n",
              " 345709,\n",
              " 812389,\n",
              " 564122,\n",
              " 409943,\n",
              " 426674,\n",
              " 601614,\n",
              " 119796,\n",
              " 649811,\n",
              " 858155,\n",
              " 429246,\n",
              " 329120,\n",
              " 722924,\n",
              " 405301,\n",
              " 315644,\n",
              " 640117,\n",
              " 884354,\n",
              " 792366,\n",
              " 842928,\n",
              " 605032,\n",
              " 317366,\n",
              " 221265,\n",
              " 128768,\n",
              " 139646,\n",
              " 446160,\n",
              " 325262,\n",
              " 277417,\n",
              " 257826,\n",
              " 407360,\n",
              " 579515,\n",
              " 769520,\n",
              " 808816,\n",
              " 628854,\n",
              " 427874,\n",
              " 110215,\n",
              " 592588,\n",
              " 856347,\n",
              " 327000,\n",
              " 858436,\n",
              " 878358,\n",
              " 195267,\n",
              " 213477,\n",
              " 574304,\n",
              " 692542,\n",
              " 910232,\n",
              " 359736,\n",
              " 790781,\n",
              " 328573,\n",
              " 586536,\n",
              " 249197,\n",
              " 166980,\n",
              " 269018,\n",
              " 726073,\n",
              " 483324,\n",
              " 442127,\n",
              " 505436,\n",
              " 929178,\n",
              " 601920,\n",
              " 481643,\n",
              " 347454,\n",
              " 634658,\n",
              " 444557,\n",
              " 919258,\n",
              " 932606,\n",
              " 299033,\n",
              " 156531,\n",
              " 109683,\n",
              " 545263,\n",
              " 823828,\n",
              " 464219,\n",
              " 316450,\n",
              " 685615,\n",
              " 957161,\n",
              " 762279,\n",
              " 296491,\n",
              " 441870,\n",
              " 649162,\n",
              " 662472,\n",
              " 681209,\n",
              " 441742,\n",
              " 403993,\n",
              " 364673,\n",
              " 238897,\n",
              " 398434,\n",
              " 852328,\n",
              " 401818,\n",
              " 105236,\n",
              " 638911,\n",
              " 716230,\n",
              " 267286,\n",
              " 411476,\n",
              " 453577,\n",
              " 206644,\n",
              " 952376,\n",
              " 717160,\n",
              " 178451,\n",
              " 325225,\n",
              " 251440,\n",
              " 785994,\n",
              " 783029,\n",
              " 880137,\n",
              " 136838,\n",
              " 191622,\n",
              " 642133,\n",
              " 156304,\n",
              " 910568,\n",
              " 776425,\n",
              " 804047,\n",
              " 788206,\n",
              " 820364,\n",
              " 819111,\n",
              " 526987,\n",
              " 851347,\n",
              " 487236,\n",
              " 708256,\n",
              " 613408,\n",
              " 905706,\n",
              " 104986,\n",
              " 223964,\n",
              " 887875,\n",
              " 428951,\n",
              " 96249,\n",
              " 236907,\n",
              " 649233,\n",
              " 191797,\n",
              " 268218,\n",
              " 237346,\n",
              " 516165,\n",
              " 447311,\n",
              " 302149,\n",
              " 738395,\n",
              " 582281,\n",
              " 719359,\n",
              " 230078,\n",
              " 858566,\n",
              " 745000,\n",
              " 739104,\n",
              " 263704,\n",
              " 185334,\n",
              " 828218,\n",
              " 585467,\n",
              " 401509,\n",
              " 457218,\n",
              " 241540,\n",
              " 790654,\n",
              " 528453,\n",
              " 852352,\n",
              " 599095,\n",
              " 353857,\n",
              " 213258,\n",
              " 408689,\n",
              " 587678,\n",
              " 687871,\n",
              " 297157,\n",
              " 123699,\n",
              " 581969,\n",
              " 414259,\n",
              " 123514,\n",
              " 895839,\n",
              " 934535,\n",
              " 409055,\n",
              " 850103,\n",
              " 162287,\n",
              " 707686,\n",
              " 942571,\n",
              " 907535,\n",
              " 832011,\n",
              " 539529,\n",
              " 680273,\n",
              " 247617,\n",
              " 543267,\n",
              " 868229,\n",
              " 789469,\n",
              " 389314,\n",
              " 620807,\n",
              " 248876,\n",
              " 866017,\n",
              " 887305,\n",
              " 459020,\n",
              " 911593,\n",
              " 682880,\n",
              " 300800,\n",
              " 571277,\n",
              " 281076,\n",
              " 458293,\n",
              " 218415,\n",
              " 173906,\n",
              " 178685,\n",
              " 200865]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}